<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title>Storing and Querying Evolving Knowledge Graphs&lt;br /&gt;on the Web</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" media="all"    href="styles/katex.css" />
  <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono" rel="stylesheet" />
  <link href="https://dokie.li/media/css/dokieli.css" media="all" rel="stylesheet" />
  <script src="https://dokie.li/scripts/dokieli.js"></script>
  <meta name="citation_title" content="Storing and Querying<br />Evolving Knowledge Graphs<br />on the Web">
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Joachim Van Herwegen" />
  <meta name="citation_author" content="Miel Vander Sande" />
  <meta name="citation_author" content="Ruben Verborgh" />
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Pieter Colpaert" />
  <meta name="citation_author" content="Erik Mannens" />
  <meta name="citation_author" content="Ruben Verborgh" />
  
  <meta name="citation_publication_date" content="2019/07/03" />
</head>


<body prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# as: https://www.w3.org/ns/activitystreams# oa: http://www.w3.org/ns/oa# ldp: http://www.w3.org/ns/ldp#" typeof="schema:CreativeWork sioc:Post prov:Entity">
  <div class="header-wrapper">
<header>
    <h1 id="storing-and-queryingbr-evolving-knowledge-graphsbr-on-the-web">Storing and Querying<br />Evolving Knowledge Graphs<br />on the Web</h1>

    <h2 id="de-opslag-en-bevragingbr-van-evoluerende-kennisgrafenbr-op-het-web">De opslag en bevraging<br />van evoluerende kennisgrafen<br />op het Web</h2>

    <ul id="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
    </ul>

    <ul id="affiliations">
      <li id="idlab">IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec</li>
    </ul>

  </header>
</div>

<section id="frontmatter">
  <section id="abstract">
    <h2>Abstract</h2>

    <!-- Context      -->
    <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.
<!-- Need         -->
Vestibulum finibus dignissim augue, id pellentesque est facilisis non.
<!-- Task         -->
Donec fringilla dolor non neque iaculis blandit.
<!-- Object       -->
Praesent aliquet eleifend iaculis.
<!-- Findings     -->
Quisque pellentesque at odio ac bibendum.
<!-- Conclusion   -->
Pellentesque imperdiet felis urna, quis facilisis lacus gravida non.
<!-- Perspectives -->
Donec quis lectus eget sem tempor tristique pellentesque in dolor.</p>

  </section>

  <section id="toc">
    <h2>Table of Contents</h2>

    <p class="todo">Generate me in ScholarMarkdown (expandable).
Auto-expand everything in print-mode.</p>

  </section>

  <section id="acronyms">
    <h2>Acronyms</h2>

    <p class="todo">Generate by serializing acronyms.csv</p>

  </section>

  <section id="preface">
    <h2>Preface</h2>

    <p class="todo">Write me</p>

  </section>

  <section id="summary">
    <h2>Summary</h2>

    <p class="todo">Write me</p>

  </section>

</section>

<main>
  <section id="introduction">
    <h2>Introduction</h2>

    <p class="todo">Context: Web+SemWeb</p>

    <h3 id="research-question">Research Question</h3>

    <p class="todo">“How to store and query evolving knowledge graphs on the Web?”
challenges: (let them correspond to the chapters)
    * The Web is highly heterogeneous
    * Experimentation requires realistic evolving data
    * Indexing evolving data is a trade-off between storage size and lookup efficiency
    * Publishing evolving data via a queryable interface is costly</p>

    <h3 id="publications">Publications</h3>

    <p class="todo">Full list of publications?</p>

    <h3 id="outline">Outline</h3>

    <p class="todo">Outline of the following chapters</p>

  </section>

  
  <section class="sub-paper">
    <h2 id="querying">Querying on the Web</h2>

    <section>
      <p class="todo">Write an introduction to this chapter</p>
    </section>

    <ul class="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
      <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/joachim_van_herwegen">Joachim Van Herwegen</a></li>
      <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/miel_vander_sande">Miel Vander Sande</a></li>
      <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
    </ul>

    <p class="published-as">Published as <a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica: a Modular SPARQL Query Engine for the Web</a></p>

    <section id="querying_abstract">
      <h3 class="no-label-increment">Abstract</h3>

      <!-- Context      -->
      <p>Query evaluation over Linked Data sources has become a complex story,
given the multitude of algorithms and techniques
for single- and multi-source querying,
as well as the heterogeneity of Web interfaces
through which data is published online.
<!-- Need         -->
Today’s query processors are insufficiently adaptable
to test multiple query engine aspects in combination,
such as evaluating the performance of a certain join algorithm
over a federation of heterogeneous interfaces.
The Semantic Web research community is in need of a flexible query engine
that allows plugging in new components
such as different algorithms,
new or experimental SPARQL features,
and support for new Web interfaces.
<!-- Task         -->
We designed and developed a Web-friendly and modular meta query engine
called <em>Comunica</em>
that meets these specifications.
<!-- Object       -->
In this article,
we introduce this query engine
and explain the architectural choices behind its design.
<!-- Findings     -->
We show how its modular nature makes it an ideal research platform
for investigating new kinds of Linked Data interfaces and querying algorithms.
<!-- Conclusion   -->
Comunica facilitates the development, testing, and evaluation
of new query processing capabilities,
both in isolation and in combination with others.
<!-- Perspectives --></p>

    </section>

    <section id="querying_introduction">
      <h3>Introduction</h3>

      <p>Linked Data on the Web exists in many shapes and forms—and
so do the processors we use to query data from one or multiple sources.
For instance,
engines that query RDF data using the <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL language</a> <span class="references">[<a href="#ref-1">1</a>]</span>
employ <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1804669.1804675"><a href="http://doi.acm.org/10.1145/1804669.1804675"><em>different algorithms</em></a></span> <span class="references">[<a href="#ref-2">2</a>, <a href="#ref-3">3</a>]</span>
and support <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-642-02184-8_2"><a href="https://doi.org/10.1007/978-3-642-02184-8_2"><em>different language extensions</em></a></span> <span class="references">[<a href="#ref-4">4</a>, <a href="#ref-5">5</a>]</span>.
Furthermore,
Linked Data is increasingly published through <em>different Web interfaces</em>,
such as
data dumps, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/DesignIssues/LinkedData.html">Linked Data documents</a> <span class="references">[<a href="#ref-6">6</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL endpoints</a> <span class="references">[<a href="#ref-7">7</a>]</span>
and <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments (TPF) interfaces</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>.
This has led to entirely different query evaluation strategies,
such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">server-side</a> <span class="references">[<a href="#ref-7">7</a>]</span>,
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf">link-traversal-based</a> <span class="references">[<a href="#ref-9">9</a>]</span>,
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">shared client–server query processing</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>,
and
client-side (by downloading data dumps and loading them locally).</p>

      <p>The resulting variety of implementations
suffers from two main problems:
a lack of <em>sustainability</em>
and a lack of <em>comparability</em>.
Alternative query algorithms and features
are typically either implemented as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf"><em>forks</em> of existing software packages</a> <span class="references">[<a href="#ref-10">10</a>, <a href="#ref-11">11</a>, <a href="#ref-12">12</a>]</span>
or as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf"><em>independent</em> engines</a> <span class="references">[<a href="#ref-13">13</a>]</span>.
This practice has limited sustainability:
forks are often not merged into the main software distribution
and hence become abandoned;
independent implementations require a considerable upfront cost
and also risk abandonment more than established engines.
Comparability is also limited:
forks based on older versions of an engine
cannot meaningfully be evaluated against newer forks,
and evaluating <em>combinations</em> of cross-implementation features—such as
different algorithms on different interfaces—is
not possible without code adaptation.
As a result, many interesting comparisons are never performed
because they are too costly to implement and maintain.
For example,
it is currently unknown
how the <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf">Linked Data Eddies algorithm</a> <span class="references">[<a href="#ref-13">13</a>]</span>
performs over a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">federation</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>
of <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48"><a href="https://arxiv.org/pdf/1608.08148.pdf">brTPF interfaces</a></span> <span class="references">[<a href="#ref-14">14</a>]</span>.
Another example is that the effects of various <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf">optimizations and extensions for TPF interfaces</a> <span class="references">[<a href="#ref-10">10</a>, <a href="#ref-11">11</a>, <a href="#ref-12">12</a>, <a href="#ref-13">13</a>, <a href="#ref-14">14</a>, <a href="#ref-15">15</a>, <a href="#ref-16">16</a>, <a href="#ref-17">17</a>]</span>
have only been evaluated in isolation,
whereas certain combinations will likely prove complementary.</p>

      <p>In order to handle the increasing heterogeneity of Linked Data on the Web,
as well as various solutions for querying it,
there is a need for a flexible and modular query engine
to experiment with all of these techniques—both separately and in combination.
In this article, we introduce <em>Comunica</em> to realize this vision.
It is a highly modular meta engine for federated SPARQL query evaluation
over heterogeneous interfaces,
including TPF interfaces, SPARQL endpoints, and data dumps.
Comunica aims to serve as a flexible research platform for
designing, implementing, and evaluating
new and existing Linked Data querying and publication techniques.</p>

      <p>Comunica differs from existing query processors on different levels:</p>

      <ol>
        <li>The <strong>modularity</strong> of the Comunica meta query engine allows for
<em>extensions</em> and <em>customization</em> of algorithms and functionality.
Users can build and fine-tune a concrete engine
by wiring the required modules through an RDF configuration document.
By publishing this document,
experiments can repeated and adapted by others.</li>
        <li>Within Comunica, multiple <strong>heterogeneous interfaces</strong> are first-class citizens. This enables federated querying over heterogeneous sources and makes it for example possible to evaluate queries over any combination of SPARQL endpoints, TPF interfaces, datadumps, or other types of interfaces.</li>
        <li>Comunica is implemented using <strong>Web-based technologies</strong> in JavaScript, which enables usage through browsers, the command line, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL protocol</a> <span class="references">[<a href="#ref-7">7</a>]</span>, or any Web or JavaScript application.</li>
      </ol>

      <p>Comunica and its default modules are publicly available
on GitHub and the npm package manager under the open-source MIT license
(canonical citation: <a href="https://zenodo.org/record/1202509#.Wq9GZhNuaHo">https:/​/​zenodo.org/record/1202509#.Wq9GZhNuaHo</a>).</p>

      <p>This article is structured as follows.
In the next section, we discuss the related work, followed by the main features of Comunica in <a href="#querying_features">Subsection 2.4</a>.
After that, we introduce the architecture of Comunica in <a href="#querying_architecture">Subsection 2.5</a>, and its implementation in <a href="#querying_implementation">Subsection 2.6</a>.
Next, we compare the performance of different Comunica configurations with the TPF Client in <a href="#querying_comparison-tpf-client">Subsection 2.7</a>.
Finally, <a href="#querying_conclusions">Subsection 2.8</a> concludes and discusses future work.</p>

    </section>

    <section id="querying_related-work">
      <h3>Related Work</h3>

      <p>In this section, we illustrate the many possible degrees of freedom for SPARQL query evaluation,
and show that they are hard to combine, which is the problem we aim to solve with Comunica.
We first discuss the SPARQL query language, its engines, and algorithms.
After that, we discuss alternative Linked Data publishing interfaces, and their connection to querying.
Finally, we discuss the software design patterns that are essential in the architecture of Comunica.</p>

      <h4 id="the-different-facets-of-sparql">The Different Facets of SPARQL</h4>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL</a> <span class="references">[<a href="#ref-1">1</a>]</span> is the W3C-recommended RDF query language.
The traditional way to implement a SPARQL query processor
is to use it as an interface to an underlying database,
resulting in a so-called <a property="schema:citation http://purl.org/spar/cito/citeAsAuthority" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/"><em>SPARQL endpoint</em></a> <span class="references">[<a href="#ref-7">7</a>]</span>.
This is similar to how an SQL interface
provides access to a relation database.
The internal storage can either be a native RDF store, e.g., AllegroGraph <span class="references">[<a href="#ref-18">18</a>]</span> and Blazegraph <span class="references">[<a href="#ref-19">19</a>]</span>,
or a non-RDF store, e.g., Virtuoso <span class="references">[<a href="#ref-20">20</a>]</span> uses a object-relational database management system.</p>

      <p>Various algorithms have been proposed for optimized SPARQL query evaluation.
Some algorithms for example use the concept of <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1804669.1804675"><a href="http://doi.acm.org/10.1145/1804669.1804675">query rewriting</a></span> <span class="references">[<a href="#ref-2">2</a>]</span> based on algebraic equivalent query operations,
others have proposed the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1145/1367497.1367578"><a href="http://doi.acm.org/10.1145/1367497.1367578">optimization of Basic Graph Pattern evaluation</a></span> <span class="references">[<a href="#ref-3">3</a>]</span> using selectivity estimation of triple patterns.</p>

      <p>In order to evaluate SPARQL queries over datasets of different storage types,
SPARQL query frameworks were developed, such as
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://jena.apache.org/">Jena (ARQ)</a> <span class="references">[<a href="#ref-21">21</a>]</span>, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdflib.readthedocs.io/en/stable/">RDFLib</a> <span class="references">[<a href="#ref-22">22</a>]</span>, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/linkeddata/rdflib.js">rdflib.js</a> <span class="references">[<a href="#ref-23">23</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/antoniogarrote/rdfstore-js">rdfstore-js</a> <span class="references">[<a href="#ref-24">24</a>]</span>.
Jena is a Java framework, RDFLib is a python package, and rdflib.js and rdfstore-js are JavaScript modules.
Jena—or more specifically the ARQ API—and RDFLib are fully <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL 1.1</a> <span class="references">[<a href="#ref-1">1</a>]</span> compliant.
rdflib.js and rdfstore-js both support a subset of SPARQL 1.1.
These SPARQL engines support in-memory models or other sources,
such as Jena TDB in the case of ARQ.
Most of the query algorithms are tightly coupled to these frameworks,
which makes swapping out query algorithms for specific query operators hard or sometimes even impossible.
Furthermore, complex things such as federated querying over heterogeneous interfaces are difficult to implement using these frameworks,
as they are not supported out-of-the-box.
This issue of modularity and heterogeneity are two of the main problems we aim to solve within Comunica.
The differences between Comunica and existing frameworks will be explained in more detail in <a href="#features"></a>.</p>

      <p>The <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments client</a></span> <span class="references">[<a href="#ref-8">8</a>]</span> (also known as Client.js or <code>ldf-client</code>) is a client-side SPARQL engine
that retrieves data over HTTP
through <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments (TPF) interfaces</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>.
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/eswc2015.pdf">Different algorithms</a> <span class="references">[<a href="#ref-10">10</a>, <a href="#ref-16">16</a>, <a href="#ref-17">17</a>]</span> for this client and
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/iswc2015-amf.pdf">TPF interface extensions</a> <span class="references">[<a href="#ref-11">11</a>, <a href="#ref-12">12</a>, <a href="#ref-14">14</a>, <a href="#ref-15">15</a>]</span> have been proposed to reduce effort of server or client in some way.
All of these efforts are however implemented and evaluated in isolation.
Furthermore, the implementations are tied to TPF interface, which makes it impossible to use them for other types of datasources and interfaces.
With Comunica, we aim to solve this by modularizing query operation implementations into separate modules,
so that they can be plugged in and combined in different ways, on top of different datasources and interfaces.</p>

      <p>With Semantic Web technologies providing the capability
to integrate data from different sources,
<em>federated query processing</em> has been an active area of research.
However, most of the existing frameworks require SPARQL endpoints on every source.
The TPF Client instead federates over TPF interfaces,
and achieves <span property="schema:citation http://purl.org/spar/cito/citesAsEvidence" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">similar performance compared to the state of the art</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>
despite its usage of a more lightweight interface.
However, no frameworks exist that enable federation over heterogeneous interfaces,
such as the federation over any combination of SPARQL endpoints and TPF interfaces.
With Comunica, we aim to fill this gap.
In addition dataset-centric approaches,
alternative methods such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf">link-traversal-based query evaluation</a> <span class="references">[<a href="#ref-9">9</a>]</span> exist
to query a web of Linked Data documents.</p>

      <h4 id="linked-data-fragments">Linked Data Fragments</h4>

      <p>In order to formally capture the heterogeneity of different Web interfaces to publish RDF data,
the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Linked Data Fragment</a></span> <span class="references">[<a href="#ref-8">8</a>]</span> (LDF) conceptual framework
uniformly characterizes responses of Web interfaces to RDF-based knowledge graphs.
The simplest type of LDF is a <em>data dump</em>—it is the response of a single HTTP requests for a complete RDF dataset.
Other types of LDFs includes responses of SPARQL endpoints,
TPF interfaces, and Linked Data documents.</p>

      <p>Existing LDF research highlights that,
when it comes to publishing datasets on the Web, there is no silver bullet:
no single interface works well in all situations,
as each one involves <span property="schema:citation http://purl.org/spar/cito/citesAsEvidence" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">trade-offs</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>.
As such, data publishers must choose the type of interface that matches their intended use case, target audience and infrastructure.
This however complicates client-side engines that need to retrieve data from the resulting heterogeneity of interfaces.
As shown by the TPF approach, interfaces can be self-descriptive and expose one or more <a property="schema:citation http://purl.org/spar/cito/cites" href="http://arxiv.org/abs/1609.07108">features</a> <span class="references">[<a href="#ref-25">25</a>]</span>,
to describe their functionality using a common vocabulary <span class="references">[<a href="#ref-26">26</a>, <a href="#ref-27">27</a>]</span>.
This allows clients without prior knowledge of the exact inputs and outputs of an interface
to discover its usage at runtime.</p>

      <p>A design goal of Comunica is to
facilitate interaction with any current and future interface
within the LDF framework,
both in single-source and federated scenarios.</p>

      <h4 id="software-design-patterns">Software Design Patterns</h4>

      <p>In the following, we discuss three software design patterns that are relevant to the modular design of the Comunica engine.</p>

      <h5 id="publishsubscribe-pattern">Publish–subscribe pattern</h5>

      <p>The <em>publish-subscribe</em> <span class="references">[<a href="#ref-28">28</a>]</span> design pattern involves passing <em>messages</em> between <em>publishers</em> and <em>subscribers</em>.
Instead of programming publishers to send messages directly to subscribers, they are programmed to <em>publish</em> messages to certain <em>categories</em>.
Subscribers can <em>subscribe</em> to these categories which will cause them to receive these published messages, without requiring prior knowledge of the publishers.
This pattern is useful for decoupling software components from each other,
and only requiring prior knowledge of message categories.
We use this pattern in Comunica for allowing different implementations of certain tasks to subscribe to task-specific buses.</p>

      <h5 id="actor-model">Actor Model</h5>

      <p>The <em>actor</em> model <span class="references">[<a href="#ref-29">29</a>]</span> was designed as a way to achieve highly parallel systems consisting of many independent <em>agents</em>
communicating using messages, similar to the publish–subscribe pattern.
An actor is a computational unit that performs a specific task, acts on messages, and can send messages to other actors.
The main advantages of the actor model are that actors can be independently made to implement certain specific tasks based on messages,
and that these can be handled asynchronously.
These characteristics are highly beneficial to the modularity that we want to achieve with Comunica.
That is why we use this pattern in combination with the publish–subscribe pattern to let each implementation of a certain task correspond to a separate actor.</p>

      <h5 id="mediator-pattern">Mediator pattern</h5>

      <p>The <em>mediator</em> <span class="references">[<a href="#ref-30">30</a>]</span> pattern is able to reduce coupling between software components that interact with each other,
and to easily change the interaction if needed.
This can be achieved by encapsulating the interaction between software components in a mediator component.
Instead of the components having to interact with each other directly,
they now interact through the mediator.
These components therefore do not require prior knowledge of each other,
and different implementations of these mediators can lead to different interaction results.
In Comunica, we use this pattern to handle actions when multiple actors are able to solve the same task,
by for example choosing the <em>best</em> actor for a task, or by combining the solutions of all actors.</p>

    </section>

    <section id="querying_features">
      <h3>Requirement analysis</h3>

      <p>In this section, we discuss the main requirements and features of the Comunica framework
as a research platform for SPARQL query evaluation.
Furthermore, we discuss each feature based on the availability in related work.
The main feature requirements of Comunica are the following:</p>

      <dl>
        <dt>SPARQL query evaluation</dt>
        <dd>The engine should be able to interpret, process and output results for SPARQL queries.</dd>
        <dt>Modularity</dt>
        <dd>Different independent modules should contain the implementation of specific tasks, and they should be combinable in a flexible framework. The configurations should be describable in RDF.</dd>
        <dt>Heterogeneous interfaces</dt>
        <dd>Different types of datasource interfaces should be supported, and it should be possible to add new types independently.</dd>
        <dt>Federation</dt>
        <dd>The engine should support federated querying over different interfaces.</dd>
        <dt>Web-based</dt>
        <dd>The engine should run in Web browsers using native Web technologies.</dd>
      </dl>

      <p>In <a href="#querying_features-comparison">Table 1</a>, we summarize the availability of these features in similar works.</p>

      <figure id="querying_features-comparison" class="table">

        <table>
          <thead>
            <tr>
              <th>Feature</th>
              <th>TPF Client</th>
              <th>ARQ</th>
              <th>RDFLib</th>
              <th>rdflib.js</th>
              <th>rdfstore-js</th>
              <th>Comunica</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>SPARQL</td>
              <td>✓(1)</td>
              <td>✓</td>
              <td>✓</td>
              <td>✓(1)</td>
              <td>✓(1)</td>
              <td>✓(1)</td>
            </tr>
            <tr>
              <td>Modularity</td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Heterogeneous interfaces</td>
              <td> </td>
              <td>✓(2,3)</td>
              <td>✓(2,3)</td>
              <td>✓(3)</td>
              <td>✓(3)</td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Federation</td>
              <td>✓</td>
              <td>✓(4)</td>
              <td>✓(4)</td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Web-based</td>
              <td>✓</td>
              <td> </td>
              <td> </td>
              <td>✓</td>
              <td>✓</td>
              <td>✓</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 1:</span> Comparison of the availability of the main features of Comunica in similar works.
(1) A subset of SPARQL 1.1 is implemented.
(2) Querying over SPARQL endpoints, other types require implementing an internal storage interface.
(3) Downloading of dumps.
(4) Federation only over SPARQL endpoints using the SERVICE keyword.</p>
        </figcaption>
      </figure>

      <h4 id="sparql-query-evaluation">SPARQL query evaluation</h4>

      <p>The recommended way of querying within RDF data, is using the SPARQL query language.
All of the discussed frameworks support at least the parsing and execution of SPARQL queries, and reporting of results.</p>

      <h4 id="modularity">Modularity</h4>

      <p>Adding new functionality or changing certain operations in Comunica should require minimal to no changes to existing code.
Furthermore, the Comunica environment should be developer-friendly, including well documented APIs and auto-generation of stub code.
In order to take full advantage of the Linked Data stack, modules in Comunica must be describable, configurable and wireable in RDF.
By registering or excluding modules from a configuration file, the user is free to choose how heavy or lightweight the query engine will be.
Comunica’s modular architecture will be explained in <a href="#querying_architecture">Subsection 2.5</a>.
ARQ, RDFLib, rdflib.js and rdfstore-js only support customization by implementing a custom query engine programmatically to handle operators.
They do not allow plugging in or out certain modules.</p>

      <h4 id="heterogeneous-interfaces">Heterogeneous interfaces</h4>

      <p>Due to the existence of different types of Linked Data Fragments for exposing Linked Datasets,
Comunica should support <em>heterogeneous</em> interfaces types, including self-descriptive Linked Data interfaces such as TPF.
This TPF interface is the only interface that is supported by the TPF Client.
Additionally, Comunica should also enable querying over other sources,
such as SPARQL endpoints and data dumps in RDF serializations.
The existing SPARQL frameworks mostly support querying against SPARQL endpoints,
local graphs, and specific storage types using an internal storage adapter.</p>

      <h4 id="federation">Federation</h4>

      <p>Next to the different type of Linked Data Fragments for exposing Linked Datasets,
data on the Web is typically spread over <em>different</em> datasets, at different locations.
As mentioned in <a href="#querying_related-work">Subsection 2.3</a>, federated query processing is a way to query over the combination of such datasets,
without having to download the complete datasets and querying over them locally.
The TPF client supports federated query evaluation over its single supported interface type, i.e., TPF interfaces.
ARQ and RDFLib only support federation over SPARQL endpoints using the SERVICE keyword.
Comunica should enable <em>combined</em> federated querying over its supported heterogeneous interfaces.</p>

      <h4 id="web-based">Web-based</h4>

      <p>Comunica must be built using native Web technologies, such as JavaScript and RDF configuration documents.
This allows Comunica to run in different kinds of environments, including Web browsers, local (JavaScript) runtime engines and command-line interfaces,
just like the TPF-client, rdflib.js and rdfstore-js.
ARQ and RDFLib are able to run in their language’s runtime and via a command-line interface, but not from within Web browsers.
ARQ would be able to run in browsers using a custom Java applet, which is not a native Web technology.</p>

    </section>

    <section id="querying_architecture">
      <h3>Architecture</h3>

      <p>In this section, we discuss the design and architecture of the Comunica meta engine,
and show how it conforms to the <em>modularity</em> feature requirement.
In summary, Comunica is collection of small modules that, when wired together,
are able to perform a certain task, such as evaluating SPARQL queries.
We first discuss the customizability of Comunica at design-time,
followed by the flexibility of Comunica at run-time.
Finally, we give an overview of all modules.</p>

      <h4 id="customizable-wiring-at-design-time-through-dependency-injection">Customizable Wiring at Design-time through Dependency Injection</h4>

      <p>There is no such thing as <em>the</em> Comunica engine,
instead, Comunica is a meta engine that can be <em>instantiated</em> into different engines based on different configurations.
Comunica achieves this customizability at design-time using the concept of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://martinfowler.com/articles/injection.html"><em>dependency injection</em></a> <span class="references">[<a href="#ref-31">31</a>]</span>.
Using a configuration file, which is created before an engine is started,
components for an engine can be <em>selected</em>, <em>configured</em> and <em>combined</em>.
For this, we use the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://componentsjs.readthedocs.io/en/latest/">Components.js</a> <span class="references">[<a href="#ref-32">32</a>]</span> JavaScript dependency injection framework,
This framework is based on semantic module descriptions and configuration files
using the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://linkedsoftwaredependencies.org/articles/describing-experiments/">Object-Oriented Components ontology</a> <span class="references">[<a href="#ref-33">33</a>]</span>.</p>

      <h5 id="description-of-individual-software-components">Description of Individual Software Components</h5>

      <p>In order to refer to Comunica components from within configuration files,
we semantically describe all Comunica components using the Components.js framework in JSON-LD <span class="references">[<a href="#ref-34">34</a>]</span>.
<a href="#querying_config-actor">Listing 1</a> shows an example of the semantic description of an RDF parser.</p>

      <h5 id="description-of-complex-software-configurations">Description of Complex Software Configurations</h5>

      <p>A specific instance of a Comunica engine
can be <em>initialized</em> using Components.js configuration files
that describe the wiring between components.
For example, <a href="#querying_config-parser">Listing 2</a> shows a configuration file of an engine that is able to parse N3 and JSON-LD-based documents.
This example shows that, due to its high degree of modularity,
Comunica can be used for other purposes than a query engine,
such as building a custom RDF parser.</p>

      <p>Since many different configurations can be created,
it is important to know which one was used for a specific use case or evaluation.
For that purpose,
the RDF documents that are used to instantiate a Comunica engine
can be <a property="schema:citation http://purl.org/spar/cito/citeAsEvidence" href="https://linkedsoftwaredependencies.org/articles/describing-experiments/">published as Linked Data</a> <span class="references">[<a href="#ref-33">33</a>]</span>.
They can then serve as provenance
and as the basis for derived set-ups or evaluations.</p>

      <figure id="querying_config-actor" class="listing">
<pre><code>{
</code><code>  &quot;@context&quot;: [ ... ],
</code><code>  &quot;@id&quot;: &quot;npmd:@comunica/actor-rdf-parse-n3&quot;,
</code><code>  &quot;components&quot;: [
</code><code>    {
</code><code>      &quot;@id&quot;:            &quot;crpn3:Actor/RdfParse/N3&quot;,
</code><code>      &quot;@type&quot;:          &quot;Class&quot;,
</code><code>      &quot;extends&quot;:        &quot;cbrp:Actor/RdfParse&quot;,
</code><code>      &quot;requireElement&quot;: &quot;ActorRdfParseN3&quot;,
</code><code>      &quot;comment&quot;:        &quot;An actor that parses Turtle-like RDF&quot;,
</code><code>      &quot;parameters&quot;: [
</code><code>        {
</code><code>          &quot;@id&quot;: &quot;caam:Actor/AbstractMediaTypedFixed/mediaType&quot;,
</code><code>          &quot;default&quot;: [ &quot;text/turtle&quot;, &quot;application/n-triples&quot; ]
</code><code>        }
</code><code>      ]
</code><code>    }
</code><code>  ]
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 1:</span> Semantic description of a component that is able to parse N3-based RDF serializations.
This component has a single parameter that allows media types to be registered that this parser is able to handle.
In this case, the component has four default media types.</p>
        </figcaption>
</figure>

      <figure id="querying_config-parser" class="listing">
<pre><code>{
</code><code>  &quot;@context&quot;: [ ... ],
</code><code>  &quot;@id&quot;: &quot;http://example.org/myrdfparser&quot;,
</code><code>  &quot;@type&quot;: &quot;Runner&quot;,
</code><code>  &quot;actors&quot;: [
</code><code>    { &quot;@type&quot;: &quot;ActorInitRdfParse&quot;,
</code><code>      &quot;mediatorRdfParse&quot;: {
</code><code>        &quot;@type&quot;: &quot;MediatorRace&quot;,
</code><code>        &quot;cc:Mediator/bus&quot;: { &quot;@id&quot;: &quot;cbrp:Bus/RdfParse&quot; }
</code><code>      } },
</code><code>    { &quot;@type&quot;: &quot;ActorRdfParseN3&quot;,
</code><code>      &quot;cc:Actor/bus&quot;: &quot;cbrp:Actor/RdfParse&quot; },
</code><code>    { &quot;@type&quot;: &quot;ActorRdfParseJsonLd&quot;,
</code><code>      &quot;cc:Actor/bus&quot;: &quot;cbrp:Actor/RdfParse&quot; },
</code><code>  ]
</code><code>}
</code></pre>
<figcaption>
          <p><span class="label">Listing 2:</span> Comunica configuration of <code>ActorInitRdfParse</code> for parsing an RDF document in an unknown serialization.
This actor is linked to a mediator with a bus containing two RDF parsers for specific serializations.</p>
        </figcaption>
</figure>

      <h4 id="flexibility-at-run-time-using-the-actormediatorbus-pattern">Flexibility at Run-time using the Actor–Mediator–Bus Pattern</h4>

      <p>Once a Comunica engine has been configured and initialized,
components can interact with each other in a flexible way using the <em>actor</em> <span class="references">[<a href="#ref-29">29</a>]</span>,
<em>mediator</em> <span class="references">[<a href="#ref-30">30</a>]</span>, and <em>publish–subscribe</em> <span class="references">[<a href="#ref-28">28</a>]</span> patterns.
Any number of <em>actor</em>, <em>mediator</em> and <em>bus</em> modules can be created,
where each actor interacts with mediators, that in turn invoke other actors that are registered to a certain bus.</p>

      <p><a href="#querying_actor-mediator-bus">Fig. 1</a> shows an example logic flow between actors through a mediator and a bus.
The relation between these components, their phases and the chaining of them will be explained hereafter.</p>

      <figure id="querying_actor-mediator-bus">
<img src="querying/img/actor-mediator-bus.svg" alt="[actor-mediator-bus pattern]" class="figure-narrow" />
<figcaption>
          <p><span class="label">Fig. 1:</span> Example logic flow where Actor 0 requires an <em>action</em> to be performed.
This is done by sending the action to the Mediator, which sends a <em>test action</em> to Actors 1, 2 and 3 via the Bus.
The Bus then sends all <em>test replies</em> to the Mediator,
which chooses the best actor for the action, in this case Actor 3.
Finally, the Mediator sends the original action to Actor 3, and returns its response to Actor 0.</p>
        </figcaption>
</figure>

      <h5 id="relation-between-actors-and-buses">Relation between Actors and Buses</h5>

      <p>Actors are the main computational units in Comunica, and buses and mediators form the <em>glue</em> that ties them together and makes them interactable.
Actors are responsible for being able to accept certain messages
via the bus to which they are subscribed,
and for responding with an answer.
In order to avoid a single high-traffic bus for all message types which could cause performance issues,
separate buses exist for different message types.
<a href="#querying_relation-actor-bus">Fig. 2</a> shows an example of how actors can be registered to buses.</p>

      <figure id="querying_relation-actor-bus">
<img src="querying/img/relation-actor-bus.svg" alt="[relation between actors and buses]" class="figure-narrow" />
<figcaption>
          <p><span class="label">Fig. 2:</span> An example of two different buses each having two subscribed actors.
The left bus has different actors for parsing triples in a certain RDF serialization to triple objects.
The right bus has actors that join query bindings streams together in a certain way.</p>
        </figcaption>
</figure>

      <h5 id="mediators-handle-actor-run-and-test-phases">Mediators handle Actor Run and Test Phases</h5>

      <p>Each mediator is connected to a single bus, and its goal is to determine and invoke the <em>best</em> actor for a certain task.
The definition of ‘<em>best</em>’ depends on the mediator, and different implementations can lead to different choices in different scenarios.
A mediator works in two phases: the <em>test</em> phase and the <em>run</em> phase.
The test phase is used to check under which conditions the action can be performed in each actor on the bus.
This phase must always come before the <em>run</em> phase, and is used to select which actor is best suited to perform a certain task under certain conditions.
If such an actor is determined, the <em>run</em> phase of a single actor is initiated.
This <em>run</em> phase takes this same type of message, and requires to <em>effectively act</em> on this message,
and return the result of this action.
<a href="#querying_run-test-phases">Fig. 3</a> shows an example of a mediator invoking a run and test phase.</p>

      <figure id="querying_run-test-phases">
<img src="querying/img/run-test-phases.svg" alt="[mediators handle actor run and test phases]" />
<figcaption>
          <p><span class="label">Fig. 3:</span> Example sequence diagram of a mediator that chooses the fastest actor
on a parse bus with two subscribed actors.
The first parser is very fast but requires a lot of memory,
while the second parser is slower, but requires less memory.
Which one is best, depends on the use case and is determined by the Mediator.
The mediator first calls the <em>tests</em> the actors for the action, and then <em>runs</em> the action using the <em>best</em> actor.</p>
        </figcaption>
</figure>

      <h4 id="modules">Modules</h4>

      <p>At the time of writing, Comunica consists of 79 different modules.
This consists of 13 buses, 3 mediator types, 57 actors and 6 other modules.
In this section, we will only discuss the most important actors and their interactions.</p>

      <p>The main bus in Comunica is the <em>query operation</em> bus, which consists of 19 different actors
that provide at least one possible implementation of the typical SPARQL operations such as quad patterns, basic graph patterns (BGPs), unions, projects, …
These actors interact with each other using streams of <em>quad</em> or <em>solution mappings</em>,
and act on a query plan expressed in in <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL algebra</a> <span class="references">[<a href="#ref-1">1</a>]</span>.</p>

      <p>In order to enable heterogeneous sources to be queried in a federated way,
we allow a list of sources, annotated by type, to be passed when a query is initiated.
These sources are passed down through the chain of query operation actors,
until the quad pattern level is reached.
At this level, different actors exist for handling a single source of a certain type,
such as TPF interfaces, SPARQL endpoints, local or remote datadumps.
In the case of multiple sources, one actor exists that implements a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">federation algorithm defined for TPF</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>,
but instead of federating over different TPF interfaces, it federates over different single-source quad pattern actors.</p>

      <p>At the end of the pipeline, different actors are available for serializing the results of a query in different ways.
For instance, there are actors for serializing the results according to
the SPARQL <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/">JSON</a> <span class="references">[<a href="#ref-35">35</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/rdf-sparql-XMLres/">XML</a> <span class="references">[<a href="#ref-36">36</a>]</span> result specifications,
but actors with more visual and developer-friendly formats are available as well.</p>

    </section>

    <section id="querying_implementation">
      <h3>Implementation</h3>

      <p>Comunica is implemented in TypeScript/JavaScript as a collection of Node modules, which are able to run in Web browsers using native Web technologies.
Comunica is available under an open license on <a href="https://zenodo.org/record/1202509#.Wq9GZhNuaHo" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​1202509#.Wq9GZhNuaHo">GitHub</a>
and on the <a href="https://www.npmjs.com/org/comunica" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​org/​comunica">NPM package manager</a>.
The 79 Comunica modules are tested thoroughly, with more than 1,200 unit tests reaching a test coverage of 100%.
In order to be compatible with existing JavaScript RDF libraries,
Comunica follows the JavaScript API specification by the <a href="https://www.w3.org/community/rdfjs/" class="mandatory" data-link-text="https:/​/​www.w3.org/​community/​rdfjs/​">RDFJS community group</a>,
and will <a href="https://www.w3.org/community/rdfjs/2018/04/23/rdf-js-the-new-rdf-and-linked-data-javascript-library/">actively be further aligned</a> within this community.
In order to encourage collaboration within the community, we extensively use the <a href="https://github.com/comunica/comunica/issues" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​issues">GitHub issue tracker</a>
for planned features, bugs and other issues.
Finally, we publish detailed <a href="https://comunica.readthedocs.io" class="mandatory" data-link-text="https:/​/​comunica.readthedocs.io">documentation</a> for the usage and development of Comunica.</p>

      <p>We provide a default Linked Data-based configuration file with all available actors for evaluating federated <em>SPARQL queries</em> over heterogeneous sources.
This allows SPARQL queries to be evaluated using a command-line tool,
from a Web service implementing the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL protocol</a> <span class="references">[<a href="#ref-7">7</a>]</span>,
within a JavaScript application,
or within the browser.
We fully implemented <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/">SPARQL 1.0</a> <span class="references">[<a href="#ref-37">37</a>]</span> and a subset of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL 1.1</a> <span class="references">[<a href="#ref-1">1</a>]</span> at the time of writing.
In future work, we intend to implement additional actors for supporting SPARQL 1.1 completely.</p>

      <p>Comunica currently supports querying over the following types of <em>heterogeneous datasources and interfaces</em>:</p>

      <ul>
        <li><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments interfaces</a></span> <span class="references">[<a href="#ref-8">8</a>]</span></li>
        <li>Quad Pattern Fragments interfaces (<a href="https://github.com/LinkedDataFragments/Server.js/tree/feature-qpf-latest" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​Server.js/​tree/​feature-​qpf-​latest">an experimental extension of TPF with a fourth graph element</a>)</li>
        <li><a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">SPARQL endpoints</a> <span class="references">[<a href="#ref-7">7</a>]</span></li>
        <li>Local and remote dataset dumps in RDF serializations.</li>
        <li><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT datasets</a> <span class="references">[<a href="#ref-38">38</a>]</span></li>
        <li><a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-demo/">Versioned OSTRICH datasets</a> <span class="references">[<a href="#ref-39">39</a>]</span></li>
      </ul>

      <p>In order to demonstrate Comunica’s ability to evaluate <em>federated</em> query evaluation over <em>heterogeneous</em> sources,
the following guide shows how you can <a href="https://gist.github.com/rubensworks/34bb69fa6c83176bce60a5e8a25051e8" class="mandatory" data-link-text="https:/​/​gist.github.com/​rubensworks/​34bb69fa6c83176bce60a5e8a25051e8">try this out in Comunica yourself</a>.</p>

      <p>Support for new algorithms, query operators and interfaces can be implemented in an external module,
without having to create a custom fork of the engine.
The module can then be <em>plugged</em> into existing or new engines that are identified by
<a href="https://github.com/comunica/comunica/blob/master/packages/actor-init-sparql/config/config-default.json" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​blob/​master/​packages/​actor-​init-​sparql/​config/​config-​default.json">RDF configuration files</a>.</p>

      <p>In the future, we will also look into adding support for other interfaces such as
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48"><a href="https://arxiv.org/pdf/1608.08148.pdf">brTPF</a></span> <span class="references">[<a href="#ref-14">14</a>]</span> for more efficient join operations
and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf">VTPF</a> <span class="references">[<a href="#ref-15">15</a>]</span> for queries over versioned datasets.</p>

    </section>

    <section id="querying_comparison-tpf-client">
      <h3>Performance Analysis</h3>

      <p>One of the goals of Comunica is to replace the TPF Client as a more <em>flexible</em> and <em>modular</em> alternative,
with at least the same <em>functionality</em> and similar <em>performance</em>.
The fact that Comunica supports multiple heterogeneous interfaces and sources as shown in the previous section
validates this flexibility and modularity, as the TPF Client only supports querying over TPF interfaces.</p>

      <p>Next to a functional completeness, it is also desired that Comunica achieves similar <em>performance</em> compared to the TPF Client.
The higher modularity of Comunica is however expected to cause performance overhead,
due to the additional bus and mediator communication, which does not exist in the TPF Client.
Hereafter, we compare the performance of the TPF Client and Comunica
and discover that Comunica has similar performance to the TPF Client.
As the main goal of Comunica is modularity, and not <em>absolute</em> performance, we do not compare with similar frameworks such as ARQ and RDFLib.
Instead, <em>relative</em> performance of evaluations using <em>the same engine</em> under <em>different configurations</em> is key for comparisons,
which will be demonstrated using Comunica hereafter.</p>

      <p>For the setup of this evaluation we used a single machine (Intel Core i5-3230M CPU at 2.60 GHz with 8 GB of RAM),
running the Linked Data Fragments server with a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT-backend</a> <span class="references">[<a href="#ref-38">38</a>]</span> and the TPF Client or Comunica,
for which the exact versions and configurations will be linked in the following workflow.
The main goal of this evaluation is to determine the performance impact of Comunica,
while keeping all other variables constant.</p>

      <p>In order to illustrate the benefit of modularity within Comunica,
we evaluate using two different configurations of Comunica.
The first configuration (<em>Comunica-sort</em>) implements a BGP algorithm that is similar to that of the original TPF Client:
it sorts triple patterns based on their estimated counts and evaluates and joins them in that order.
The second configuration (<em>Comunica-smallest</em>) implements a simplified version of this BGP algorithm that does not sort <em>all</em> triple patterns in a BGP,
but merely picks the triple pattern with the smallest estimated count to evaluate on each recursive call, leading to slightly different query plans.</p>

      <p>We used the following <a about="#evaluation-workflow" content="Comunica evaluation workflow" href="#evaluation-workflow" property="rdfs:label" rel="cc:license" resource="https://creativecommons.org/licenses/by/4.0/">evaluation workflow</a>:</p>

      <ol id="evaluation-workflow" property="schema:hasPart" resource="#evaluation-workflow" typeof="opmw:WorkflowTemplate">
<li id="workflow-data" about="#workflow-data" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Generate a <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-319-11964-9_13"><a href="http://dx.doi.org/10.1007/978-3-319-11964-9_13">WatDiv</a></span> <span class="references">[<a href="#ref-40">40</a>]</span> dataset with scale factor=100.</p>
        </li>
<li id="workflow-queries" about="#workflow-queries" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Generate the corresponding default WatDiv <a href="https://github.com/comunica/test-comunica/tree/ISWC2018/sparql/watdiv-10M" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​test-​comunica/​tree/​ISWC2018/​sparql/​watdiv-​10M">queries</a> with query-count=5.</p>
        </li>
<li id="workflow-tpf-server" about="#workflow-tpf-server" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Install <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-config.jsonld" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​ldf-​availability-​experiment-​config.jsonld">the server software configuration</a>, implementing the <a href="https://www.hydra-cg.com/spec/latest/triple-pattern-fragments/" class="mandatory" data-link-text="https:/​/​www.hydra-​cg.com/​spec/​latest/​triple-​pattern-​fragments/​">TPF specification</a>, with its <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-setup.ttl">dependencies</a>.</p>
        </li>
<li id="workflow-tpf-client" about="#workflow-tpf-client" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Install <a href="https://github.com/LinkedDataFragments/Client.js" class="mandatory" data-link-text="https:/​/​github.com/​LinkedDataFragments/​Client.js">the TPF Client software</a>, implementing the <a href="https://www.w3.org/TR/sparql11-protocol">SPARQL 1.1 protocol</a>, with its <a href="https://linkedsoftwaredependencies.org/raw/ldf-availability-experiment-client.ttl" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​ldf-​availability-​experiment-​client.ttl">dependencies</a>.</p>
        </li>
<li id="workflow-tpf-run" about="#workflow-tpf-run" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Execute the generated WatDiv queries 3 times on the TPF Client, after doing a warmup run, and record the execution times <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-ldf.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​ldf.csv">results</a>.</p>
        </li>
<li id="workflow-comunica-sort" about="#workflow-comunica-srt" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Install <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/config-sort.json" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​config-​sort.json">the Comunica software configuration</a>, implementing the <a href="https://www.w3.org/TR/sparql11-protocol">SPARQL 1.1 protocol</a>, with its <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/comunica-npm.ttl" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​comunica-​npm.ttl">dependencies</a>, using the <em>Comunica-sort</em> algorithm.</p>
        </li>
<li id="workflow-comunica-run-sort" about="#workflow-comunica-run-sort" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Execute the generated WatDiv queries 3 times on the Comunica client, after doing a warmup run, and record the <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-comunica-sort.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​comunica-​sort.csv">execution times</a>.</p>
        </li>
<li id="workflow-comunica-smallest" about="#workflow-comunica-smallest" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Update the Comunica installation to use a new <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/config/config-smallest.json" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​config/​config-​smallest.json">configuration</a> supporting the <em>Comunica-smallest</em> algorithm.</p>
        </li>
<li id="workflow-comunica-run-smallest" about="#workflow-comunica-run-smallest" typeof="opmw:WorkflowTemplateProcess" rel="opmw:isStepOfTemplate" resource="#evaluation-workflow" property="rdfs:label">
          <p>Execute the generated WatDiv queries 3 times on the Comunica client, after doing a warmup run, and record the <a href="https://raw.githubusercontent.com/comunica/test-comunica/master/results/watdiv-comunica.csv" class="mandatory" data-link-text="https:/​/​raw.githubusercontent.com/​comunica/​test-​comunica/​master/​results/​watdiv-​comunica.csv">execution times</a>.</p>
        </li>
</ol>

      <figure id="querying_performance-average">
<center>
<img src="querying/img/avg.svg" alt="[performance-average]" class="plot" />
<img src="querying/img/avg_c23.svg" alt="[performance-average]" class="plot" />
</center>
<figcaption>
          <p><span class="label">Fig. 4:</span> Average query evaluation times for the TPF Client, Comunica-sort, and Comunica-smallest for all queries (shorter is better).
C2 and C3 are shown separately because of their higher evaluation times.</p>
        </figcaption>
</figure>

      <p>The results from <a href="#querying_performance-average">Fig. 4</a> show that Comunica is able to achieve similar performance compared to the TPF Client.
Concretely, both Comunica variants are faster for 11 queries, and slower for 9 queries.
However, the difference in evaluation times is in most cases very small,
and are caused by implementation details, as the implemented algorithms are equivalent.
Contrary to our expectations, the performance overhead of Comunica’s modularity is negligible.
Comunica therefore improves upon the TPF Client in terms of <em>modularity</em> and <em>functionality</em>, and achieves similar <em>performance</em>.</p>

      <p>These results also illustrate the simplicity of comparing different algorithms inside Comunica.
In this case, we compared an algorithm that is similar to that of the original TPF Client with a simplified variant.
The results show that the performance is very similar, but the original algorithm (Comunica-sort) is faster in most of the cases.
It is however not always faster, as illustrated by query C1, where Comunica-sort is almost a second slower than Comunica-smallest.
In this case, the heuristic algorithm of the latter was able to come up with a slightly better query plan.
Our goal with this result is to show that Comunica can easily be used to compare such different algorithms,
where future work can focus on smart mediator algorithms to choose the best BGP actor in each case.</p>

    </section>

    <section id="querying_conclusions">
      <h3>Conclusions</h3>

      <p>In this work, we introduced Comunica as a highly modular meta engine for federated SPARQL query evaluation over heterogeneous interfaces.
Comunica is thereby the first system that accomplishes the Linked Data Fragments vision of a client that is able to query over heterogeneous interfaces.
Not only can Comunica be used as a client-side SPARQL engine, it can also be customized to become a more lightweight engine and perform more specific tasks,
such as for example only evaluating BGPs over Turtle files,
evaluating the efficiency of different join operators,
or even serve as a complete server-side SPARQL query endpoint that aggregates different datasources.
In future work, we will look into supporting supporting alternative (non-semantic) query languages as well, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://facebook.github.io/graphql/October2016/">GraphQL</a> <span class="references">[<a href="#ref-41">41</a>]</span>.</p>

      <p>If you are a Web researcher, then Comunica is the ideal research platform
for investigating new Linked Data publication interfaces,
and for experimenting with different query algorithms.
New modules can be implemented independently without having to fork existing codebases.
The modules can be combined with each other using an RDF-based configuration file
that can be instantiated into an actual engine through dependency injection.
However, the target audience is broader than just the research community.
As Comunica is built on Linked Data and Web technologies,
and is extensively documented and has a ready-to-use API,
developers of RDF-consuming (Web) applications can also make use of the platform.
In the future, we will continue <a href="https://github.com/comunica/comunica/wiki/Sustainability-Plan" class="mandatory" data-link-text="https:/​/​github.com/​comunica/​comunica/​wiki/​Sustainability-​Plan">maintaining</a>
and developing Comunica and intend to support and collaborate with future researchers on this platform.</p>

      <p>The introduction of Comunica will trigger a <em>new generation of Web querying research</em>.
Due to its flexibility and modularity,
existing areas can be <em>combined</em> and <em>evaluated</em> in more detail,
and <em>new promising areas</em> that remained covered so far will be exposed.</p>

    </section>

    <div class="subfooter">
  <section id="querying_acknowledgements">
        <h3 class="no-label-increment">Acknowledgements</h3>

        <p>The described research activities were funded by Ghent University, imec,
Flanders Innovation &amp; Entrepreneurship (AIO), and the European Union.
Ruben Verborgh is a postdoctoral fellow of the Research Foundation – Flanders.</p>

      </section>

</div>
  </section>
  
  <section class="sub-paper">
    <h2 id="generating">Generating Evolving Data</h2>

    <section>
      <p class="todo">Write an introduction to this chapter</p>
    </section>

    <ul class="authors">
      <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
      <li><a href="https://pietercolpaert.be/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://pietercolpaert.be/#me">Pieter Colpaert</a></li>
      <li><a href="https://www.linkedin.com/in/erikmannens" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/erik_mannens">Erik Mannens</a></li>
      <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
    </ul>

    <p class="published-as">Published as <a href="https://www.rubensworks.net/raw/publications/2018/podigg.pdf">Generating Public Transport Data based on Population Distributions for RDF Benchmarking</a></p>

    <section id="generating_abstract">
      <h3 class="no-label-increment">Abstract</h3>

      <!--context-->
      <p>When benchmarking RDF data management systems such as public transport route planners,
system evaluation needs to happen under various realistic circumstances,
which requires a wide range of datasets with different properties.
Real-world datasets are almost ideal, as they offer these realistic circumstances,
but they are often hard to obtain and inflexible for testing.
For these reasons, synthetic dataset generators are typically preferred
over real-world datasets due to their intrinsic flexibility.
Unfortunately, many synthetic dataset that are generated within benchmarks are insufficiently realistic,
raising questions about the generalizability of benchmark results to real-world scenarios.
<!--need-->
In order to benchmark geospatial and temporal RDF data management systems
such as route planners
with sufficient external validity and depth,
<!--task-->
we designed PoDiGG,
a highly configurable generation algorithm for synthetic public transport datasets
with realistic geospatial and temporal characteristics
comparable to those of their real-world variants.
The algorithm is inspired by real-world public transit network design
and scheduling methodologies.
<!--object-->
This article discusses the design and implementation of PoDiGG
and validates the properties of its generated datasets.
<!--findings-->
Our findings show that the generator achieves a sufficient level of realism,
based on the existing coherence metric and new metrics we introduce specifically for the public transport domain.
<!--conclusions-->
Thereby, PoDiGG
provides a flexible foundation for benchmarking RDF data management systems with geospatial and temporal data.</p>

    </section>

    <section id="generating_introduction">
      <h3>Introduction</h3>

      <p>The <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">Resource Description Framework (RDF)</a> <span class="references">[<a href="#ref-42">42</a>]</span> and Linked Data <span class="references">[<a href="#ref-43">43</a>]</span> technologies enable distributed use and management of semantic data models.
Datasets with an interoperable domain model can be stored and queried by different data owners in different ways.
In order to discover the strengths and weaknesses of different storage and querying possibilities,
data-driven benchmarks with different sizes of datasets and varying characteristics can be used.</p>

      <p>Regardless of whether existing data-driven benchmarks use real or synthetic datasets,
the <em>external validity</em> of their results can be too limited,
which makes a generalization to other datasets difficult.
Real datasets, on the one hand, are often only scarcely available for testing,
and only cover very specific scenarios,
such that not all aspects of systems can be assessed.
Synthetic datasets, on the other hand, are typically generated by
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/Bizer-Schultz-Berlin-SPARQL-Benchmark-IJSWIS.pdf"><em>mimicking algorithms</em></a> <span class="references">[<a href="#ref-44">44</a>, <a href="#ref-45">45</a>, <a href="#ref-46">46</a>, <a href="#ref-47">47</a>]</span>,
which are not always sufficiently realistic <span class="references">[<a href="#ref-48">48</a>]</span>.
Features that are relevant for real-world datasets may not be tested.
As such, conclusions drawn from existing benchmarks
do not always apply to the envisioned real-world scenarios.
One way to get the best of both worlds
is to design mimicking algorithms that generate realistic synthetic datasets.</p>

      <p>The <em>public transport</em> domain provides data with both geospatial and temporal properties,
which makes this an especially interesting source of data for benchmarking.
Its representation as Linked Data is valuable because
1) of the many shared entities, such as stops, routes and trips, across different existing datasets on the Web.
2) These entities can be distributed over different datasets
and 3) benefit from interlinking for the improvement of discoverability.
Synthetic public transport datasets are particularly important and needed
in cases where public transport route planning algorithms are evaluated.
The Linked Connections framework <span class="references">[<a href="#ref-49">49</a>]</span> and Connection Scan Algorithm <span class="references">[<a href="#ref-50">50</a>]</span>
are examples of such public transport route planning systems.
Because of the limited availability of real-world datasets with desired properties,
these systems were evaluated with only a very low number of datasets, respectively one and three datasets.
A synthetic public transport dataset generator would make it easier for researchers
to include a higher number of realistic datasets with various properties in their evaluations,
which would be beneficial to the discovery of new insights from the evaluations.
Network size, network sparsity and temporal range are examples of such properties,
and different combinations of them may not always be available in real datasets,
which motivates the need for generating synthetic, but realistic datasets with these properties.</p>

      <p>Not only are public transport datasets useful for benchmarking route planning systems,
they are also highly useful for benchmarking <a property="schema:citation http://purl.org/spar/cito/cites" href="https://link.springer.com/content/pdf/10.1007/978-3-642-35176-1_19.pdf">geospatial</a> <span class="references">[<a href="#ref-51">51</a>, <a href="#ref-52">52</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/1526709.1526856">temporal</a> <span class="references">[<a href="#ref-53">53</a>, <a href="#ref-54">54</a>]</span> RDF systems
due to the intrinsic geospatial and temporal properties of public transport datasets.
While synthetic dataset generators already exist in the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://dx.doi.org/10.1007/978-3-642-41338-4_22">geospatial and temporal domain</a> <span class="references">[<a href="#ref-55">55</a>, <a href="#ref-56">56</a>]</span>,
no systems exist yet that focus on realism, and specifically look into the generation of public transport datasets.
As such, the main topic that we address in this work, is solving the need for realistic public transport datasets
with geospatial and temporal characteristics,
so that they can be used to benchmark RDF data management and route planning systems.
More specifically, we introduce a mimicking algorithm for generating realistic public transport data,
which is the main contribution of this work.</p>

      <p>We observed a significant correlation between transport networks and the population distributions of their geographical areas,
which is why population distributions are the driving factor within our algorithm.
The cause of this correlation is obvious, considering transport networks are frequently used to transport people,
but other – possibly independent – factors exist that influence transport networks as well,
like certain points of interest such as tourist attractions and shopping areas.
Our algorithm is subdivided into five sequential steps,
inspired by existing methodologies from the domains of <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">public transit planning</a> <span class="references">[<a href="#ref-57">57</a>]</span>
as a means to improve the realism of the algorithm’s output data.
These steps include the creation of a geospatial region, the placement of stops, edges and routes, and the scheduling of trips.
We provide an implementation of this algorithm, with different parameters to configure the algorithm.
Finally, we confirm the realism of datasets that are generated by this algorithm
using the existing generic structuredness metric <span class="references">[<a href="#ref-48">48</a>]</span>
and new metrics that we introduce, which are specific to the public transport domain.
The notable difference of this work compared to other synthetic dataset generators
is that our generation algorithm specializes in generating public transit networks,
while other generators either focus on other domains, or aim to be more general-purpose.
Furthermore, our algorithm is based on population distributions and existing methodologies from public transit network design.</p>

      <p>In the next section, we introduce the related work on dataset generation,
followed by the background on public transit network design, and transit feed formats in <a href="#generating_public-transit-background">Subsection 3.4</a>.
In <a href="#generating_research-question">Subsection 3.5</a>, we introduce the main research question and hypothesis of this work.
Next, our algorithm is presented in <a href="#generating_methodology">Subsection 3.6</a>, followed by its implementation in <a href="#generating_implementation">Subsection 3.7</a>.
In <a href="#generating_evaluation">Subsection 3.8</a>, we present the evaluation of our implementation,
followed by a discussion and conclusion in <a href="#generating_discussion">Subsection 3.9</a> and <a href="#generating_conclusions">Subsection 3.10</a>.</p>
    </section>

    <section id="generating_related-work">
      <h3>Related Work</h3>

      <p>In this section, we present the related work on spatiotemporal and RDF dataset generation,</p>

      <p>Spatiotemporal database systems store instances that are described using an identifier, a spatial location and a timestamp.
In order to evaluate spatiotemporal indexing and querying techniques with datasets,
automatic means exist to generate such datasets with predictable characteristics <span class="references">[<a href="#ref-58">58</a>]</span>.</p>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">Brinkhoff</a> <span class="references">[<a href="#ref-59">59</a>]</span> argues that moving objects tend to follow a predefined network.
Using this and other statements, he introduces a spatiotemporal dataset generator.
Such a network can be anything over which certain objects can move,
ranging from railway networks to air traffic connections.
The proposed parameter-based generator restricts the existence of the spatiotemporal objects to
a predefined time period <span class="kdmath">$\lbrack t_\text{min},t_\text{max})$</span>.
It is assumed that each edge in the network has a maximum allowed speed and capacity
over which objects can move at a certain speed.
The eventual speed of each object is defined by the maximum speed of its class,
the maximum allowed speed of the edge, and the congestion of the edge based on its capacity.
Furthermore, external events that can impact the movement of the objects, such as weather conditions,
are represented as temporal grids over the network, which apply a <em>decreasing factor</em> on the maximum speed of the objects in certain areas.
The existence of each object that is generated starts at a certain timestamp,
which is determined by a certain function,
and <em>dies</em> when it arrives at its destination.
The starting node of an object can be chosen based on three approaches:</p>

      <ul>
        <li><strong>dataspace-oriented approaches</strong>: Selecting the nearest node to a position picked from a two-dimensional distribution function that maps positions to nodes.</li>
        <li><strong>region-based approaches</strong>: Improvement of the data-space oriented approach where the data space is represented as a collection of cells, each having a certain chance of being the place of a starting node.</li>
        <li><strong>network-based approaches</strong>: Selection of a network node based on a one-dimensional distribution function that assigns a chance to each node.</li>
      </ul>

      <p>Determining the destination node using one of these approaches leads to non-satisfying results.
Instead, the destination is derived from the preferred length of a route.
Each route is determined as the fastest path to a destination, weighed by the external events.
Finally, the results are reported as either textual output, insertion into a database or a figure of the generated objects.
Compared to our work, this approach assumes a predefined network,
while our algorithm also includes the generation of the network.
For our work, we reuse the concepts of object speed and region-based node selection with relation to population distributions.</p>

      <p>In order to improve the testability of Information Discovery Systems,
a <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">generic synthetic dataset generator</a> <span class="references">[<a href="#ref-60">60</a>]</span> was developed
that is able to generate synthetic data based on declarative graph definitions.
This graph is based on objects, attributes and relationships between them.
The authors propose to generate new instances, such as people, based on a set of dependency rules.
They introduce three types of dependencies for the generation of instances:</p>

      <ul>
        <li><strong>independent</strong>: Attribute values that are independent of other instances and attributes.</li>
        <li><strong>intra-record (horizontal) dependencies</strong>: Attribute values depending on other values of the same instance.</li>
        <li><strong>inter-record (vertical) dependencies</strong>: Relationships between different instances.</li>
      </ul>

      <p>Their engine is able to accept such dependencies as part of a semantic graph definition,
and iteratively create new instances to form a synthetic dataset.
This tool however outputs non-RDF CSV files, which makes it impossible to directly use this system for
the generation of public transport datasets in RDF using existing ontologies.
For our public transport use case, individual entities such as stops, stations and connections
would be possible to generate up to a certain level using this declarative tool.
However, due to the underlying relation to population distributions
and specific restrictions for resembling real datasets,
declarative definitions are too limited.</p>

      <p>The need for benchmarking RDF data management systems is illustrated by the existence of the Linked Data Benchmark Council <span class="references">[<a href="#ref-61">61</a>]</span>
and the <a href="http://project-hobbit.eu/" class="mandatory" data-link-text="http:/​/​project-​hobbit.eu/​">HOBBIT H2020 EU project</a> for benchmarking of Big Linked Data.
RDF benchmarks are typically based on certain datasets that are used as input to the tested systems.
Many of these datasets are not always very closely related to real datasets <span class="references">[<a href="#ref-48">48</a>]</span>,
which may result in conclusions drawn from benchmarking results that do not translate to system behaviours in realistic settings.</p>

      <p>Duan et al. <span class="references">[<a href="#ref-48">48</a>]</span> argue that the realism of an RDF dataset can be measured
by comparing the <em>structuredness</em> of that dataset with a realistic equivalent.
The authors show that real-world datasets are typically less structured than their synthetic counterparts,
which can results in significantly different benchmarking results,
since this level of structuredness can have an impact on how certain data is stored in RDF data management systems.
This is because these systems may behave differently on datasets with different levels of structuredness,
as they can have certain optimizations for some cases.
In order to measure this structuredness, the authors introduce the <em>coherence</em>
metric of a dataset <span class="kdmath">$D$</span> with a type system <span class="kdmath">$\mathcal{T}$</span> that can be calculated as follows:</p>

      <div class="kdmath">$$
\begin{aligned}
    CH(\mathcal{T}, D) = \sum_{\forall{T \in \mathcal{T}}} WT(CV(T, D)) * CV(T, D)
\end{aligned}
$$</div>

      <p>The type system <span class="kdmath">$\mathcal{T}$</span> contains all the RDF types that are present in a dataset.
<span class="kdmath">$CV(T, D)$</span> represents the <em>coverage</em> of a type <span class="kdmath">$T$</span> in a dataset <span class="kdmath">$D$</span>,
and is calculated as the fraction of type instances that set a value for all its properties.
The factor <span class="kdmath">$WT(CV(T, D))$</span> is used to weight this sum,
so that the coherence is always a value between 0 and 1, with 1 representing a perfect structuredness.
A maximal coherence means that all instances in the dataset have values for all possible properties in the type system,
which is for example the case in relational databases without optional values.
Based on this metric, the authors introduce a generic method for creating variants of real datasets
with different sizes while maintaining a similar structuredness.
The authors describe a method to calculate the coverage value of this dataset,
which has been <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1700/paper-02.pdf">implemented as a procedure in the Virtuoso RDF store</a> <span class="references">[<a href="#ref-47">47</a>]</span>.
As the goal of our work is to generate <em>realistic</em> RDF public transport datasets,
we will use this metric to compare the realism of generated datasets with real datasets.
As this high-level metric is used to define <em>realism</em> over any kind of RDF dataset,
we will introduce new metrics to validate the realism for specifically the case of public transport datasets.</p>

    </section>

    <section id="generating_public-transit-background">
      <h3>Public Transit Background</h3>

      <p>In this section, we present background on public transit planning that is essential to this work.
We discuss existing public transit network planning methodologies
and formats for exchanging transit feeds.</p>

      <h4 id="public-transit-planning">Public Transit Planning</h4>

      <p>The domain of public transit planning entails the design of public transit networks,
rostering of crews, and all the required steps inbetween.
The goal is to maximize the quality of service for passengers while minimizing the costs for the operator.
Given a public demand and a topological area, this planning process aims to obtain routes, timetables and vehicle and crew assignment.
A survey about 69 existing public transit planning approaches
shows that these processes are typically subdivided into <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">five sequential steps</a> <span class="references">[<a href="#ref-57">57</a>]</span>:</p>

      <ol>
        <li><strong>route design</strong>, the placement of transit routes over an existing network.</li>
        <li><strong>frequencies setting</strong>, the temporal instantiation of routes based on the available vehicles and estimated demand.</li>
        <li><strong>timetabling</strong>, the calculation of arrival and departure times at each stop based on estimated demand.</li>
        <li><strong>vehicle scheduling</strong>, vehicle assignment to trips.</li>
        <li><strong>crew scheduling and rostering</strong>, the assignment of drivers and additional crew to trips.</li>
      </ol>

      <p>In this paper, we only consider the first three steps for our mimicking algorithm,
which lead to all the required information
that is of importance to passengers in a public transit schedule.
We present the three steps from this survey in more detail hereafter.</p>

      <p>The first step, route design, requires the topology of an area and public demand as input.
This topology describes the network in an area, which contains possible stops and edges between these stops.
Public demand is typically represented as <em>origin-destination</em> (OD) matrices,
which contain the number of passengers willing to go from origin stops to destination stops.
Given this input, routes are designed based on the following <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">objectives</a> <span class="references">[<a href="#ref-57">57</a>]</span>:</p>

      <ul>
        <li><strong>area coverage</strong>: The percentage of public demand that can be served.</li>
        <li><strong>route and trip directness</strong>: A metric that indicates how much the actual trips from passengers deviate from the shortest path.</li>
        <li><strong>demand satisfaction</strong>: How many stops are close enough to all origin and destination points.</li>
        <li><strong>total route length</strong>: The total distance of all routes, which is typically minimized by operators.</li>
        <li><strong>operator-specific objectives</strong>: Any other constraints the operator has, for example the shape of the network.</li>
        <li><strong>historical background</strong>: Existing routes may influence the new design.</li>
      </ul>

      <p>The next step is the setting of frequencies, which is based on the routes from the previous step, public demand and vehicle availability.
The main objectives in this step are based on the following <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">metrics</a> <span class="references">[<a href="#ref-57">57</a>]</span>:</p>

      <ul>
        <li><strong>demand satisfaction</strong>: How many stops are serviced frequently enough to avoid overcrowding and long waiting times.</li>
        <li><strong>number of line runs</strong>: How many times each line is serviced – a trade-off between the operator’s aim for minimization and the public demand for maximization.</li>
        <li><strong>waiting time bounds</strong>: Regulation may put restrictions on minimum and maximum waiting times between line runs.</li>
        <li><strong>historical background</strong>: Existing frequencies may influence the new design.</li>
      </ul>

      <p>The last important step for this work is timetabling, which takes the output from the previous steps as input, together with the public demand.
The objectives for this step are the following:</p>

      <ul>
        <li><strong>demand satisfaction</strong>: Total travel time for passengers should be minimized.</li>
        <li><strong>transfer coordination</strong>: Transfers from one line to another at a certain stop should be taken into account during stop waiting times, including how many passengers are expected to transfer.</li>
        <li><strong>fleet size</strong>: The total amount of available vehicles and their usage will influence the timetabling possibilities.</li>
        <li><strong>historical background</strong>: Existing timetables may influence the new design.</li>
      </ul>

      <h4 id="transit-feed-formats">Transit Feed Formats</h4>

      <p>The de-facto standard for public transport time schedules is
the <a href="https://developers.google.com/transit/gtfs/" class="mandatory" data-link-text="https:/​/​developers.google.com/​transit/​gtfs/​">General Transit Feed Specification (GTFS)</a>.
GTFS is an exchange format for transit feeds, using a series of CSV files contained in a zip file.
The specification uses the following terminology to define the rules for a public transit system:</p>

      <ul>
        <li><strong>Stop</strong> is a geospatial location where vehicles stop and passengers can get on or off, such as platform 3 in the train station of Brussels.</li>
        <li><strong>Stop time</strong> indicates a scheduled arrival and departure time at a certain stop.</li>
        <li><strong>Route</strong> is a time-independent collection of stops, describing the sequence of stops a certain vehicle follows in a certain public transit line. For example the train route from Brussels to Ghent.</li>
        <li><strong>Trip</strong> is a collection of stops with their respective stop times, such as the route from Brussels to Ghent at a certain time.</li>
      </ul>

      <p>The \zip file is put online by a public transit operator, to be downloaded by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1007/978-3-642-02094-0_7">route planning</a> <span class="references">[<a href="#ref-62">62</a>]</span> software.
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/1227161.1227166">Two models are commonly used to then extract these rules into a graph</a> <span class="references">[<a href="#ref-63">63</a>]</span>.
In a <em>time-expanded model</em>, a large graph is modeled with arrivals and departures as nodes and edges connect departures and arrivals together.
The weights on these edges are constant.
In a <em>time-dependent model</em>, a smaller graph is modeled in which vertices are physical stops and edges are transit connections between them.
The weights on these edges change as a function of time.
In both models, Dijkstra and Dijkstra-based algorithms can be used to calculate routes.</p>

      <p>In contrast to these two models, the <em>Connection Scan Algorithm</em> <span class="references">[<a href="#ref-50">50</a>]</span>
takes an ordered array representation of <em>connections</em> as input.
A connection is the actual departure time at a stop and an arrival at the next stop.
These connections can be given a IRI, and described using RDF, using the Linked Connections <span class="references">[<a href="#ref-49">49</a>]</span> ontology.
For this base algorithm and its derivatives, a connection object is the smallest building block of a transit schedule.</p>

      <p>In our work, generated public transport networks and time schedules
can be serialized to both the GTFS format, and RDF datasets using the Linked Connections ontology.</p>

    </section>

    <section id="generating_research-question">
      <h3>Research Question</h3>

      <p>In order to generate public transport networks and schedules,
we start from the hypothesis that both
are correlated with the population distribution within the same area.
More populated areas are expected to have more nearby and more frequent access to public transport,
corresponding to the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">recurring demand satisfaction objective in public transit planning</a> <span class="references">[<a href="#ref-57">57</a>]</span>.
When we calculate the correlation
between the distribution of stops in an area and its population distribution,
we discover a positive correlation of 0.439 for Belgium and 0.459 for the Netherlands (<em>p</em>-values in both cases &lt; 0.00001),
thereby validating our hypothesis with a confidence of 99%.
Because of the continuous population variable and the binary variable indicating whether or not there is a stop,
the correlation is calculated using the <a href="https://github.com/PoDiGG/podigg-evaluate/blob/master/stats/correlation.r" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​evaluate/​blob/​master/​stats/​correlation.r">point-biserial correlation coefficient</a>.
For the calculation of these correlations, we ignored the population value outliers.
Following this conclusion, our mimicking algorithm will use such population distributions as input,
and derive public transport networks and trip instances.</p>

      <p>The main objective of a mimicking algorithm is to create <em>realistic</em> data,
so that it can be used to by benchmarks to evaluate systems under realistic circumstances.
We will measure dataset realism in high-level by comparing the levels of structuredness
of real-world datasets and their synthetic variants
using the <em>coherence metric</em> introduced by Duan et al. <span class="references">[<a href="#ref-48">48</a>]</span>.
Furthermore, we will measure the realism of different characteristics within public transport datasets,
such as the location of stops, density of the network of stops, length of routes or the frequency of connections.
We will quantify these aspects by measuring the distance of each aspect between real and synthetic datasets.
These dataset characteristics will be linked with potential evaluation metrics within RDF data management systems,
and tasks to evaluate them.
This generic coherence metric together with domain-specific metrics will provide a way to evaluate dataset realism.</p>

      <p>Based on this, we introduce the following research question for this work:</p>

      <blockquote id="generating_researchquestion">
        <p>Can population distribution data be used to generate realistic synthetic public transport networks and scheduling?</p>
      </blockquote>

      <p>We provide an answer to this question by first introducing an
algorithm for generating public transport networks and their scheduling based on population distributions in <a href="#generating_methodology">Subsection 3.6</a>.
After that, we validate the realism of datasets that were generated using an implementation of this algorithm in <a href="#generating_evaluation">Subsection 3.8</a>.</p>

    </section>

    <section id="generating_methodology">
      <h3>Method</h3>

      <p>In order to formulate an answer to our research question,
we designed a mimicking algorithm that generates realistic synthetic public transit feeds.
We based it on techniques from the domains of public transit planning, spatiotemporal and RDF dataset generation.
We reuse the route design, frequencies setting and timetabling steps from the domain public transit planning,
but prepend this with a network generation phase.</p>

      <p><a href="#generating_fig:methodology:datamodel">Fig. 5</a> shows the model of the generated public transit feeds,
with connections being the primary data element.</p>

      <figure id="generating_fig:methodology:datamodel">
<img src="generating/img/datamodel.svg" alt="PoDiGG data model" />
<figcaption>
          <p><span class="label">Fig. 5:</span> The resources (rectangle), literals (dashed rectangle) and properties (arrows) used to model the generated public transport data.
Node and text colors indicate vocabularies.</p>
        </figcaption>
</figure>

      <p>We consider different properties in this model based on the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">independent, intra-record or inter-record dependency rules</a> <span class="references">[<a href="#ref-60">60</a>]</span>, as discussed in <a href="#generating_related-work">Subsection 3.3</a>.
The arrival time in a connection can be represented as a fully intra-record dependency,
because it depends on the time it departed and the stops it goes between.
The departure time in a connection is both an intra-record and inter-record dependency,
because it depends on the stop at which it departs,
but also on the arrival time of the connection before it in the trip.
Furthermore, the delay value can be seen as an inter-record dependency,
because it is influenced by the delay value of the previous connection in the trip.
Finally, the geospatial location of a stop depends on the location of its parent station,
so this is also an inter-record dependency.
All other unmentioned properties are independent.</p>

      <p>In order to generate data based on these dependency rules, our algorithm is subdivided in five steps:</p>

      <ol>
        <li><strong>Region</strong>: Creation of a two-dimensional area of cells annotated with population density information.</li>
        <li><strong>Stops</strong>: Placement of stops in the area.</li>
        <li><strong>Edges</strong>: Connecting stops using edges.</li>
        <li><strong>Routes</strong>: Generation of routes between stops by combining edges.</li>
        <li><strong>Trips</strong>: Scheduling of timely trips over routes by instantiating connections.</li>
      </ol>

      <p>These steps are not fully sequential, since stop generation is partially executed before and after edge generation.
The first three steps are required to generate a network,
step 4 corresponds to the route design step in public transit planning
and step 5 corresponds to both the frequencies setting and timetabling.
These steps are explained in the following subsections.</p>

      <h4 id="region">Region</h4>

      <p>In order to create networks, we sample geographic regions in which such networks exist as two-dimensional matrices.
The resolution is defined as a configurable number of cells per square of one latitude by one longitude.
Network edges are then represented as links between these cells.
Because our algorithm is population distribution-based, each cell contains a population density.
These values can either be based on real population information from countries,
or this can be generated based on certain statistical distributions.
For the remainder of this paper, we will reuse the population distribution from Belgium as a running example, as illustrated in <a href="#generating_fig:methodology:region">Fig. 6</a>.</p>

      <figure id="generating_fig:methodology:region">
<img src="generating/img/region.png" alt="Heatmap of the population distribution in Belgium" />
<figcaption>
          <p><span class="label">Fig. 6:</span> Heatmap of the population distribution in Belgium, which is illustrated for each cell
as a scale going from white (low), to red (medium) and black (high).
The actual placement of train stops are indicated as green points.</p>
        </figcaption>
</figure>

      <h4 id="stops">Stops</h4>

      <p>Stop generation is divided into two steps.
First, stops are placed based on population values,
then the edge generation step is initiated
after which the second phase of stop generation is executed where additional stops are created based on the generated edges.</p>

      <p><strong>Population-based</strong>
For the initial placement of stops, our algorithm only takes a population distribution as input.
The algorithm iteratively selects random cells in the two-dimensional area, and tags those cells as stops.
To make it <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">region-based</a> <span class="references">[<a href="#ref-59">59</a>]</span>,
the selection uses a weighted Zipf-like-distribution, where cells with high population values have a higher chance
of being picked than cells with lower values.
The shape of this Zipf curve can be scaled to allow for different stop distributions to be configured.
Furthermore, a minimum distance between stops can be configured, to avoid situations where all stops are placed in highly population areas.</p>

      <p><strong>Edge-based</strong>
Another stop generation phase exists after the edge generation
because real transit networks typically show line artifacts for stop placement.
<a href="#generating_fig:methodology:stopplacementgs">Fig. 8</a> shows the actual train stops in Belgium, which clearly shows line structures.
Stop placement after the first generation phase results can be seen in <a href="#generating_fig:methodology:stopplacementp1">Fig. 9</a>,
which does not show these line structures.
After the second stop generation phase, these line structures become more apparent as can be seen in <a href="#generating_fig:methodology:stopplacementp2">Fig. 15</a>.</p>

      <figure id="generating_fig:methodology:stopplacement">

<figure id="generating_fig:methodology:stopplacementgs" class="subfigure">
<img src="generating/img/stops_gs.png" alt="Real stops" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 7:</span> Real stops with line structures.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp1" class="subfigure">
<img src="generating/img/stops_parameterized_1.png" alt="Generation phase step 1" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 9:</span> Synthetic stops after the first stop generation phase without line structures.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp2" class="subfigure">
<img src="generating/img/stops_parameterized_2.png" alt="Generation phase step 2" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 15:</span> Synthetic stops after the second stop generation phase with line structures.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Placement of train stops in Belgium, each dot represents one stop.</p>
        </figcaption>
</figure>

      <p>In this second stop generation phase,
edges are modified so that sufficiently populated areas will be included in paths formed by edges,
as illustrated by <a href="#generating_fig:methodology:stopsphase2">Fig. 11</a>.
Random edges will iteratively be selected, weighted by the edge length measured as
Euclidian distance.
(The Euclidian distance based on geographical coordinates is always used to calculate distances in this work.)
On each edge, a random cell is selected weighed by the population value in the cell.
Next, a weighed random point in a certain area around this point is selected.
This selected point is marked as a stop, the original edge is removed and two new edges are added,
marking the path between the two original edge nodes and the newly selected node.</p>

      <figure id="generating_fig:methodology:stopsphase2">

<figure id="generating_fig:methodology:stopsphase2_1" class="subfigure">
<img src="generating/img/stops_phase2_1.svg" alt="Real stops" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 11:</span> Selecting a weighted random point on the edge.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopsphase2_2" class="subfigure">
<img src="generating/img/stops_phase2_2.svg" alt="Generation phase step 1" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 13:</span> Defining an area around the selected point.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopsphase2_3" class="subfigure">
<img src="generating/img/stops_phase2_3.svg" alt="Generation phase step 2" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 14:</span> Choosing a random point within the area, weighted by population value.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:stopplacementp2" class="subfigure">
<img src="generating/img/stops_phase2_4.svg" alt="Generation phase step 2" style="width: 20%" />
<figcaption>
            <p><span class="label">Fig. 15:</span> Modify edges so that the path includes this new point.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Illustration of the second phase of stop generation where edges are modified to include sufficiently populated areas in paths.</p>
        </figcaption>
</figure>

      <h4 id="edges">Edges</h4>
      <p>The next phase in public transit network generation connects the stops that were generated in the previous phase with edges.
In order to simulate real transit network structures, we split up this generation phase into three sequential steps.
In the first step, clusters of nearby stops are formed, to lay the foundation for short-distance routes.
Next, these local clusters are connected with each other, to be able to form long-distance routes.
Finally, a cleanup step is in place to avoid abnormal edge structures in the network.</p>

      <p><strong>Short-distance</strong>
The formation of clusters with nearby stations is done using agglomerative hierarchical clustering.
Initially, each stop is part of a seperate cluster, where each cluster always maintains its centroid.
The clustering step will iteratively try to merge two clusters with their centroid distance below a certain threshold.
This threshold will increase for each iteration, until a maximum value is reached.
The maximum distance value indicates the maximum inter-stop distance for forming local clusters.
When merging two clusters, an edge is added between the closest stations from the respective clusters.
The center location of the new cluster is also recalculated before the next iteration.</p>

      <p><strong>Long-distance</strong>
At this stage, we have several clusters of nearby stops.
Because all stops need to be reachable from all stops, these separate clusters also need to be connected.
This problem is related to the domain of route planning over public transit networks,
in which networks can be decomposed into smaller clusters of nearby stations to improve the efficiency of route planning.
Each cluster contains one or more <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1137/1.9781611974317.2"><a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611974317.2"><em>border stations</em></a></span> <span class="references">[<a href="#ref-64">64</a>]</span>, which are the only points
through which routes can be formed between different clusters.
We reuse this concept of border stations, by iteratively picking a random cluster,
identifying its closest cluster based on the minimal possible stop distance, and connecting their border stations using a new edge.
After that, the two clusters are merged.
The iteration will halt when all clusters are merged and there is only one connected graph.</p>

      <p><strong>Cleanup</strong>
The final cleanup step will make sure that the number of stops that are connected by only one edge are reduced.
In real train networks, the majority of stations are connected with at least more than one other stations.
The two earlier generation steps however generate a significant number of <em>loose stops</em>,
which are connected with only a single other stop with a direct edge.
In this step, these loose stops are identified, and an attempt is made to connect them to other nearby stops as shown in <a href="#generating_alg:methodology:loosestops">Algorithm 1</a>.
For each loose stop, this is done by first identifying the direction of the single edge of the loose stop on line 18.
This direction is scaled by the radius in which to look for stops, and defines the stepsize for the loop the starts on line 20.
This loop starts from the loose stop and iteratively moves the search position in the defined direction, until it finds a random stop in the radius,
or the search distance exceeds the average distance of between the stops in the neighbourhood of this loose stop.
This random stop from line 22 can be determined
by finding all stations that have a distance to the search point that is below the radius, and picking a random stop from this collection.
If such a stop is found, an edge is added from our loose stop to this stop.</p>

      <figure id="generating_alg:methodology:loosestops" class="algorithm numbered">
<pre><code>FUNCTION RemoveLooseStops(S, E, N, O, r)
</code><code>  INPUT:
</code><code>    Set of stops S
</code><code>    Set of edges E between the stops from S
</code><code>    Maximum number N of closest stations to consider
</code><code>    Maximum average distance O around a stop to be considered a loose station
</code><code>    Radius r in which to look for stops.
</code><code>FOREACH s in S with degree of 1 w.r.t. E DO
</code><code>    sx = x coordinate of s
</code><code>    sy = y coordinate of s
</code><code>    C = N closest stations to s in S excluding s
</code><code>    c = closest station to s in S excluding s
</code><code>    cx = x coordinate of c
</code><code>    cy = y coordinate of c
</code><code>    a = average distance between each pair of stops in C
</code><code>    IF a &lt;= O and C not empty THEN
</code><code>        dx= (sx - cx) * r
</code><code>        dy= (sy - cy) * r
</code><code>        ox = sx; oy = sy
</code><code>        WHILE distance between o and s &lt; a DO
</code><code>            ox += dx; oy += dy
</code><code>            s&#39; = random station around o with radius a * r
</code><code>            IF s&#39; exists
</code><code>                add edge between s and s&#39; to E and continue next for-loop iteration
</code></pre>
<figcaption>
          <p><span class="label">Algorithm 1:</span> Reduce the number of loose stops by adding additional edges.</p>
        </figcaption>
</figure>

      <p><a href="#generating_fig:methodology:edges">Fig. 16</a> shows an example of these three steps.
After this phase, a network with stops and edges is available, and the actual transit planning can commence.</p>

      <figure id="generating_fig:methodology:edges">
 
<figure id="generating_fig:methodology:edges1" class="subfigure">
<img src="generating/img/edges_1.svg" alt="Real stops" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 16:</span> Formation of local clusters.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:edges2" class="subfigure">
<img src="generating/img/edges_2.svg" alt="Generation phase step 1" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 18:</span> Connecting clusters through border stations.</p>
          </figcaption>
</figure>

<figure id="generating_fig:methodology:edges3" class="subfigure">
<img src="generating/img/edges_3.svg" alt="Generation phase step 2" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 19:</span> Cleanup of loose stops.</p>
          </figcaption>
</figure>

<figcaption>
          <p>Example of the different steps in the edges generation algorithm.</p>
        </figcaption>
</figure>

      <p><strong>Generator Objectives</strong>
The main guaranteed objective of the edge generator is that the stops form a single connected transit network graph.
This is to ensure that all stops in the network can be reached from any other stop using at least one path through the network.</p>

      <h4 id="routes">Routes</h4>

      <p>Given a network of stops and edges, this phase generates routes over the network.
This is done by creating short and long distance routes in two sequential steps.</p>

      <p><strong>Short-distance</strong>
The goal of the first step is to create short routes where vehicles deliver each passed stop.
This step makes sure that all edges are used in at least one route,
this ensures that each stop can at least be reached from each other stop with one or more transfers to another line.
The algorithm does this by first determining a subset of the largest stops in the network, based on the population value.
The shortest path from each large stop to each other large stop through the network is determined.
if this shortest path is shorter than a predetermined value in terms of the number of edges,
then this path is stored as a route, in which all passed stops are considered as actual stops in the route.
For each edge that has not yet been passed after this, a route is created by iteratively
adding unpassed edges to the route that are connected to the edge until an edge is found that has already been passed.</p>

      <p><strong>Long-distance</strong>
In the next step, longer routes are created, where the transport vehicle not necessarily halts at each passed stop.
This is done by iteratively picking two stops from the list of largest stops using the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">network-based method</a> <span class="references">[<a href="#ref-59">59</a>]</span> with each stop having an equal chance to be selected.
A heuristical shortest path algorithm is used to determine a route between these stops.
This algorithm searches for edges in the geographical direction of the target stop.
This is done to limit the complexity of finding long paths through potentially large networks.
A random amount of the largest stops on the path are selected, where the amount
is a value between a minimum and maximum preconfigured route length.
This iteration ends when a predetermined number of routes are generated.</p>

      <p><strong>Generator Objectives</strong>
This algorithm takes into account the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">objectives of route design</a> <span class="references">[<a href="#ref-57">57</a>]</span>, as discussed in <a href="#generating_related-work">Subsection 3.3</a>.
More specifically, by first focusing on the largest stops, a minimal level of <em>area coverage</em> and <em>demand satisfaction</em> is achieved,
because the largest stops correspond to highly populated areas, which therefore satisfies at least a large part of the population.
By determining the shortest path between these largest stops, the <em>route and trip directness</em> between these stops is optimal.
Finally, by not instantiating all possible routes over the network, the <em>total route length</em> is limited to a reasonable level.</p>

      <h4 id="generating_subsec-methodology-trips">Trips</h4>

      <p>A time-agnostic transit network with routes has been generated in the previous steps.
In this final phase, we temporally instantiate routes
by first determining starting times for trips,
after which the following stop times can be calculated based on route distances.
Instead of generating explicit timetables, as is done in typical transit scheduling methodologies,
we create fictional rides of vehicles.
In order to achieve realistic trip times, we approximate real trip time distributions,
with the possibility to encounter delays.</p>

      <p>As mentioned before in <a href="#generating_related-work">Subsection 3.3</a>, each consecutive pair of start and stop time in a trip over an edge corresponds to a connection.
A connection can therefore be represented as a pair of timestamps,
a link to the edge representing the departure and arrival stop,
a link to the trip it is part of,
and its index within this trip.</p>

      <p><strong>Trip Starting Times</strong>
The trips generator iteratively creates new connections until a predefined number is reached.
For each connection, a random route is selected with a larger chance of picking a long route.
Next, a random start time of the connection is determined.
This is done by first picking a random day within a certain range.
After that, a random hour of the day is determined using a preconfigured distribution.
This distribution is derived from the public logs of <a href="https://hello.irail.be" class="mandatory" data-link-text="https:/​/​hello.irail.be">iRail</a>,
a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www2016.net/proceedings/companion/p873.pdf">route planning API in Belgium</a> <span class="references">[<a href="#ref-65">65</a>]</span>.
A seperate hourly distribution is used for weekdays and weekends, which is chosen depending on the random day that was determined.</p>

      <p><strong>Stop Times</strong>
Once the route and the starting time have been determined, different stop times across the trip can be calculated.
For this, we take into account the following factors:</p>

      <ul>
        <li>Maximum vehicle speed <span class="kdmath">$\omega$</span>, preconfigured constant.</li>
        <li>Vehicle acceleration <span class="kdmath">$\varsigma$</span>, preconfigured constant.</li>
        <li>Connection distance <span class="kdmath">$\delta$</span>, Euclidian distance between stops in network.</li>
        <li>Stop size <span class="kdmath">$\sigma$</span>, derived from population value.</li>
      </ul>

      <p>For each connection in the trip, the time it takes for a vehicle to move between the two stops over a certain distance is calculated
using the formula in <a href="#generating_math:methodology:distanceduration">Equation 3</a>.
<a href="#generating_math:methodology:timetomaxspeed">Equation 1</a> calculates the required time to reach maximum speed
and <a href="#generating_math:methodology:distancetomaxspeed">Equation 2</a> calculates the required distance to reach maximum speed.
This formula simulates the vehicle speeding up until its maximum speed, and slowing down again until it reaches its destination.
When the distance is too short, the vehicle will not reach its maximum speed,
and just speeds up as long as possible until is has to slow down again to stop in time.</p>

      <figure id="generating_math:methodology:timetomaxspeed" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    T_\omega &= \omega / \varsigma
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 1:</span> Time to reach maximum speed.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:methodology:distancetomaxspeed" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    \delta_\omega &= T_\omega^2 \cdot \varsigma
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 2:</span> Distance to reach maximum speed.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:methodology:distanceduration" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\begin{cases}
    2T_\omega + (\delta - 2 \delta_\omega) / \omega &\text{ if } \delta_\omega < \delta / 2 \\
    \sqrt{2\delta / \varsigma} &\text{ otherwise}
\end{cases}
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 3:</span> Duration for a vehicle to move between two stops.</p>
        </figcaption>
      </figure>

      <p>Not only the connection duration, but also the waiting times of the vehicle at each stop are important for determining the stop times.
These are calculated as a constant minimum waiting time together with a waiting time that increases for larger stop sizes <span class="kdmath">$\sigma$</span>,
this increase is determined by a predefined growth factor.</p>

      <p><strong>Delays</strong>
Finally, each connection in the trip will have a certain chance to encounter a delay.
When a delay is applicable, a delay value is randomly chosen within a certain range.
Next to this, also a cause of the delay is determined from a preconfigured list.
These causes are based on the Traffic Element Events from the <a href="https://transportdisruption.github.io/" class="mandatory" data-link-text="https:/​/​transportdisruption.github.io/​">Transport Disruption ontology</a>,
which contains a number of events that are not planned by the network operator such as strikes, bad weather or animal collisions.
Different types of delays can have a different impact factor of the delay value,
for instance, simple delays caused by rush hour would have a lower impact factor than a major train defect.
Delays are carried over to next connections in the trip, with again a chance of encountering additional delay.
Furthermore, these delay values can also be reduced when carried over to the next connection by a certain predetermined factor,
which simulates the attempt to reduce delays by letting vehicles drive faster.</p>

      <p><strong>Generator Objectives</strong>
For trip generation, we take into account several objectives from the
setting of frequencies and timetabling from <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">transit planning</a> <span class="references">[<a href="#ref-57">57</a>]</span>.
By instantiating more long distance routes, we aim to increase <em>demand satisfaction</em> as much as possible,
because these routes deliver busy and populated areas, and the goal is to deliver these more frequently.
Furthermore, by taking into account realistic time distributions for trip instantiation, we also adhere to this objective.
Secondly, by ensuring waiting times at each stop that are longer for larger stations,
the <em>transfer coordination</em> objective is taken into account to some extent.</p>

    </section>

    <section id="generating_implementation">
      <h3>Implementation</h3>

      <p>In this section, we discuss the implementation details of PoDiGG, based on the generator algorithm introduced in <a href="#generating_methodology">Subsection 3.6</a>.
PoDiGG is split up into two parts: the main PoDiGG generator, which outputs GTFS data, and PoDiGG-LC,
which depends on the main generator to output RDF data.
Serialization in RDF using existing ontologies, such as the <a href="http://vocab.gtfs.org/terms" class="mandatory" data-link-text="http:/​/​vocab.gtfs.org/​terms">GTFS</a>
and <a href="http://semweb.mmlab.be/ns/linkedconnections" class="mandatory" data-link-text="http:/​/​semweb.mmlab.be/​ns/​linkedconnections">Linked Connections ontologies</a>,
allows this inherently linked data to be used within RDF data management systems,
where it can for instance be used for benchmarking purposes.
Providing output in GTFS will allow this data to be used directly within all systems
that are able to handle transit feeds, such as route planning systems.
The two generator parts will be explained hereafter, followed by a section on how the generator can be configured using various parameters.</p>

      <h4 id="podigg">PoDiGG</h4>

      <p>The main requirement of our system is the ability to generate realistic
public transport datasets using the mimicking algorithm that was introduced in <a href="#generating_methodology">Subsection 3.6</a>.
This means that given a population distribution of a certain region,
the system must be able to design a network of routes,
and determine timely trips over this network.</p>

      <p>PoDiGG is implemented to achieve this goal. It is written in JavaScript using Node.js,
and is available under an open license on <a href="https://github.com/PoDiGG/podigg" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg">GitHub</a>.
In order to make installation and usage more convenient,
PoDiGG is available as a Node module on the <a href="https://www.npmjs.com/package/podigg" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​package/​podigg">NPM package manager</a>
and as a Docker image on <a href="https://hub.docker.com/r/podigg/podigg/" class="mandatory" data-link-text="https:/​/​hub.docker.com/​r/​podigg/​podigg/​">Docker Hub</a> to easily run on any platform.
Every sub-generator that was explained in <a href="#generating_methodology">Subsection 3.6</a>, is implemented as a separate module.
This makes PoDiGG highly modifiable and composable, because different implementations of sub-generators can easily be added and removed.
Furthermore, this flexible composition makes it possible to use real data instead of certain sub-generators.
This can be useful for instance when a certain public transport network is already available, and only the trips and connections need to be generated.</p>

      <p>We designed PoDiGG to be highly configurable
to adjust the characteristics of the generated output across different levels,
and to define a certain <em>seed</em> parameter for producing deterministic output.</p>

      <p>All sub-generators store generated data in-memory, using list-based data structures directly corresponding to the GTFS format.
This makes GTFS serialization a simple and efficient process.
<a href="#generating_table:implementation:gtfsfiles">Table 2</a> shows the GTFS files that are generated by the different PoDiGG modules.
This table does not contain references to the region and edges generator,
because they are only used internally as prerequisites to the later steps.
All required files are created to have a valid GTFS dataset.
Next to that, the optional file for exceptional service dates is created.
Furthermore, <code>delays.txt</code> is created, which is not part of the GTFS specification.
It is an extension we provide in order to serialize delay information about each connection in a trip.
These delays are represented in a CSV file containing columns for referring to a connection in a trip,
and contains delay values in milliseconds and a certain reason per connection arrival and departure,
as shown in <a href="#generating_listing:example:delays">Listing 3</a>.</p>

      <figure id="generating_table:implementation:gtfsfiles" class="table">

        <table>
          <thead>
            <tr>
              <th>File</th>
              <th>Generator</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong><code>agency.txt</code></strong></td>
              <td><em>Constant</em></td>
            </tr>
            <tr>
              <td><strong><code>stops.txt</code></strong></td>
              <td>Stops</td>
            </tr>
            <tr>
              <td><strong><code>routes.txt</code></strong></td>
              <td>Routes</td>
            </tr>
            <tr>
              <td><strong><code>trips.txt</code></strong></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><strong><code>stop_times.txt</code></strong></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><strong><code>calendar.txt</code></strong></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><code>calendar_dates.txt</code></td>
              <td>Trips</td>
            </tr>
            <tr>
              <td><code>delays.txt</code></td>
              <td>Trips</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 2:</span> The GTFS files that are written by PoDiGG, with their corresponding sub-generators that are responsible for generating the required data.
The files in bold refer to files that are required by the GTFS specification.</p>
        </figcaption>
      </figure>

      <figure id="generating_listing:example:delays" class="listing">
<pre><code>trip_id,stop_sequence,delay_departure,delay_arrival,delay_departure_reason         ,delay_arrival_reason
</code><code>100_4  ,0            ,0              ,1405754      ,                               ,td:RepairWork
</code><code>100_6  ,0            ,0              ,1751671      ,                               ,td:BrokenDownTrain
</code><code>100_6  ,1            ,1751671        ,1553820      ,td:BrokenDownTrain             ,td:BrokenDownTrain
</code><code>100_7  ,0            ,2782295        ,0            ,td:TreeAndVegetationCuttingWork,</code></pre>
<figcaption>
          <p><span class="label">Listing 3:</span> Sample of a <code>delays.txt</code> file in a GTFS dataset.</p>
        </figcaption>
</figure>

      <p>In order to easily observe the network structure in the generated datasets,
PoDiGG will always produce a figure accompanying the GTFS dataset.
<a href="#generating_fig:generated_example">Fig. 20</a> shows an example of such a visualization.</p>

      <figure id="generating_fig:generated_example">
<img src="generating/img/generated_example.png" alt="Visualization of a generated public transport network based on Belgium's population distribution" />
<figcaption>
          <p><span class="label">Fig. 20:</span> Visualization of a generated public transport network based on Belgium’s population distribution.
Each route has a different color, and dark route colors indicate more frequent trips over them than light colors.
The population distribution is illustrated for each cell as a scale going from white (low), to red (medium) and black (high).
<a href="https://linkedsoftwaredependencies.org/raw/podigg/gen.png" class="mandatory" data-link-text="https:/​/​linkedsoftwaredependencies.org/​raw/​podigg/​gen.png">Full image</a></p>
        </figcaption>
</figure>

      <p>Because the generation of large datasets can take a long time depending on the used parameters,
PoDiGG has a logging mechanism, which provides continuous feedback to the user about the current status and progress of the generator.</p>

      <p>Finally, PoDiGG provides the option to derive realistic public transit queries over the generated network,
aimed at testing the load of route planning systems.
This is done by iteratively selecting two random stops weighed by their size
and choosing a random starting time based on the same time distribution as discussed in <a href="#generating_subsec:methodology:trips"></a>.
This is serialized to a <a href="https://github.com/linkedconnections/benchmark-belgianrail#transit-schedules" class="mandatory" data-link-text="https:/​/​github.com/​linkedconnections/​benchmark-​belgianrail#transit-​schedules">JSON format</a>
that was introduced for benchmarking the Linked Connections route planner <span class="references">[<a href="#ref-49">49</a>]</span>.</p>

      <h4 id="podigg-lc">PoDiGG-LC</h4>

      <p>PoDiGG-LC is an extension of PoDiGG, that outputs data in Turtle/RDF using the ontologies shown in <a href="#generating_fig:methodology:datamodel">Fig. 5</a>.
It is also implemented in JavaScript using Node.js, and available under an open license on <a href="https://github.com/PoDiGG/podigg-lc" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​lc">GitHub</a>.
PoDiGG-LC is also available as a Node module on <a href="https://www.npmjs.com/package/podigg-lc" class="mandatory" data-link-text="https:/​/​www.npmjs.com/​package/​podigg-​lc">NPM</a>
and as a Docker image on <a href="https://hub.docker.com/r/podigg/podigg-lc/" class="mandatory" data-link-text="https:/​/​hub.docker.com/​r/​podigg/​podigg-​lc/​">Docker Hub</a>.
For this, we extended the <a href="https://github.com/PoDiGG/gtfs2lc" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​gtfs2lc">GTFS-LC tool</a> that is able
to convert GTFS datasets to RDF using the Linked Connections and GTFS ontologies.
The original tool serializes a minimal subset of the GTFS data, aimed at being used for Linked Connections route planning over connections.
Our extension also serializes trip, station and route instances, with their relevant interlinking.
Furthermore, our GTFS extension for representing delays is also supported, and is serialized
using a new <a href="http://semweb.datasciencelab.be/ns/linked-connections-delay/" class="mandatory" data-link-text="http:/​/​semweb.datasciencelab.be/​ns/​linked-​connections-​delay/​">Linked Connections Delay ontology</a> that we created.</p>

      <h4 id="configuration">Configuration</h4>

      <p>PoDiGG accepts a wide range of parameters that can be used to configure properties of the different sub-generators.
<a href="#generating_table:implementation:params">Table 3</a> shows an overview of the parameters, grouped by each sub-generator.
PoDiGG and PoDiGG-LC accept these <a href="https://github.com/PoDiGG/podigg#parameters" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg#parameters">parameters</a>
either in a JSON configuration file or via environment variables.
Both PoDiGG and PoDiGG-LC produce deterministic output for identical sets of parameters,
so that datasets can easily be reproduced given the configuration.
The <em>seed</em> parameter can be used to introduce pseudo-randomness into the output.</p>

      <figure id="generating_table:implementation:params" class="table">
<table>
    <tr>
        <th></th>
        <th>Name</th>
        <th>Default Value</th>
        <th>Description</th>
    </tr>

    <tr class="row-sep-above">
        <td></td>
        <td><code>seed</code></td>
        <td><code>1</code></td>
        <td>The random seed</td>
    </tr>

    <tr class="row-sep-above">
        <td rowspan="4" class="rotate">Region</td>
        <td><code>region_generator</code></td>
        <td><code>isolated</code></td>
        <td>Name of a region generator. (isolated, noisy or region)</td>
    </tr>
    <tr>
        <td><code>lat_offset</code></td>
        <td><code>0</code></td>
        <td>Value to add with all generated latitudes</td>
    </tr>
    <tr>
        <td><code>lon_offset</code></td>
        <td><code>0</code></td>
        <td>Value to add with all generated longitudes</td>
    </tr>
    <tr>
        <td><code>cells_per_latlon</code></td>
        <td><code>100</code></td>
        <td>How many cells go in 1 latitude/longitude</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="9" class="rotate">Stops</td>
        <td><code>stops</code></td>
        <td><code>600</code></td>
        <td>How many stops should be generated</td>
    </tr>
    <tr>
        <td><code>min_station_size</code></td>
        <td><code>0.01</code></td>
        <td>Minimum cell population value for a stop to form</td>
    </tr>
    <tr>
        <td><code>max_station_size</code></td>
        <td><code>30</code></td>
        <td>Maximum cell population value for a stop to form</td>
    </tr>
    <tr>
        <td><code>start_stop_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large population cells as stops</td>
    </tr>
    <tr>
        <td><code>min_interstop_distance</code></td>
        <td><code>1</code></td>
        <td>Minimum distance between stops in number of cells</td>
    </tr>
    <tr>
        <td><code>factor_stops_post_edges</code></td>
        <td><code>0.66</code></td>
        <td>Factor of stops to generate after edges</td>
    </tr>
    <tr>
        <td><code>edge_choice_power</code></td>
        <td><code>2</code></td>
        <td>Power for selecting longer edges to generate stops on</td>
    </tr>
    <tr>
        <td><code>stop_around_edge_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large population cells around edges</td>
    </tr>
    <tr>
        <td><code>stop_around_edge_radius</code></td>
        <td><code>2</code></td>
        <td>Radius in number of cells around an edge to select points</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="7" class="rotate">Edges</td>
        <td><code>max_intracluster_distance</code></td>
        <td><code>100</code></td>
        <td>Maximum distance between stops in one cluster</td>
    </tr>
    <tr>
        <td><code>max_intracluster_distance_growthfactor</code></td>
        <td><code>0.1</code></td>
        <td>Power for clustering with more distant stops</td>
    </tr>
    <tr>
        <td><code>post_cluster_max_intracluster_distancefactor</code></td>
        <td><code>1.5</code></td>
        <td>Power for connecting a stop with multiple stops</td>
    </tr>
    <tr>
        <td><code>loosestations_neighbourcount</code></td>
        <td><code>3</code></td>
        <td>Neighbours around a loose station that should define its area</td>
    </tr>
    <tr>
        <td><code>loosestations_max_range_factor</code></td>
        <td><code>0.3</code></td>
        <td>Maximum loose station range relative to the total region size</td>
    </tr>
    <tr>
        <td><code>loosestations_max_iterations</code></td>
        <td><code>10</code></td>
        <td>Maximum iteration number to try to connect one loose station</td>
    </tr>
    <tr>
        <td><code>loosestations_search_radius_factor</code></td>
        <td><code>0.5</code></td>
        <td>Loose station neighbourhood size factor</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="5" class="rotate">Routes</td>
        <td><code>routes</code></td>
        <td><code>1000</code></td>
        <td>The number of routes to generate</td>
    </tr>
    <tr>
        <td><code>largest_stations_fraction</code></td>
        <td><code>0.05</code></td>
        <td>The fraction of stops to form routes between</td>
    </tr>
    <tr>
        <td><code>penalize_station_size_area</code></td>
        <td><code>10</code></td>
        <td>The area in which stop sizes should be penalized</td>
    </tr>
    <tr>
        <td><code>max_route_length</code></td>
        <td><code>10</code></td>
        <td>Maximum number of edges for a route in the macro-step</td>
    </tr>
    <tr>
        <td><code>min_route_length</code></td>
        <td><code>4</code></td>
        <td>Minimum number of edges for a route in the macro-step</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="15" class="rotate">Connections</td>
        <td><code>time_initial</code></td>
        <td><code>0</code></td>
        <td>The initial timestamp (ms)</td>
    </tr>
    <tr>
        <td><code>time_final</code></td>
        <td><code>24 * 3600000</code></td>
        <td>The final timestamp (ms)</td>
    </tr>
    <tr>
        <td><code>connections</code></td>
        <td><code>30000</code></td>
        <td>Number of connections to generate</td>
    </tr>
    <tr>
        <td><code>stop_wait_min</code></td>
        <td><code>60000</code></td>
        <td>Minimum waiting time per stop</td>
    </tr>
    <tr>
        <td><code>stop_wait_size_factor</code></td>
        <td><code>60000</code></td>
        <td>Waiting time to add multiplied by station size</td>
    </tr>
    <tr>
        <td><code>route_choice_power</code></td>
        <td><code>2</code></td>
        <td>Power for selecting longer routes for connections</td>
    </tr>
    <tr>
        <td><code>vehicle_max_speed</code></td>
        <td><code>160</code></td>
        <td>Maximum speed of a vehicle in km/h</td>
    </tr>
    <tr>
        <td><code>vehicle_speedup</code></td>
        <td><code>1000</code></td>
        <td>Vehicle speedup in km/(h$^2$)</td>
    </tr>
    <tr>
        <td><code>hourly_weekday_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Hourly connection chances for weekdays</td>
    </tr>
    <tr>
        <td><code>hourly_weekend_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Hourly connection chances for weekend days</td>
    </tr>
    <tr>
        <td><code>delay_chance</code></td>
        <td><code>0</code></td>
        <td>Chance for a connection delay</td>
    </tr>
    <tr>
        <td><code>delay_max</code></td>
        <td><code>3600000</code></td>
        <td>Maximum delay</td>
    </tr>
    <tr>
        <td><code>delay_choice_power</code></td>
        <td><code>1</code></td>
        <td>Power for selecting larger delays</td>
    </tr>
    <tr>
        <td><code>delay_reasons</code></td>
        <td><code>...<sup>2</sup></code></td>
        <td>Default reasons and chances for delays</td>
    </tr>
    <tr>
        <td><code>delay_reduction_duration_fraction</code></td>
        <td><code>0.1</code></td>
        <td>Maximum part of connection duration to subtract for delays</td>
    </tr>
    
    <tr class="row-sep-above">
        <td rowspan="7" class="rotate">Queryset</td>
        <td><code>start_stop_choice_power</code></td>
        <td><code>4</code></td>
        <td>Power for selecting large starting stations</td>
    </tr>
    <tr>
        <td><code>query_count</code></td>
        <td><code>100</code></td>
        <td>The number of queries to generate</td>
    </tr>
    <tr>
        <td><code>time_initial</code></td>
        <td><code>0</code></td>
        <td>The initial timestamp</td>
    </tr>
    <tr>
        <td><code>time_final</code></td>
        <td><code>24 * 3600000</code></td>
        <td>The final timestamp</td>
    </tr>
    <tr>
        <td><code>max_time_before_departure</code></td>
        <td><code>3600000</code></td>
        <td>Minimum number of edges for a route in the macro-step</td>
    </tr>
    <tr>
        <td><code>hourly_weekday_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Chance for each hour to have a connection on a weekday</td>
    </tr>
    <tr>
        <td><code>hourly_weekend_distribution</code></td>
        <td><code>...<sup>1</sup></code></td>
        <td>Chance for each hour to have a connection on a weekend day</td>
    </tr>
</table>
<figcaption>
          <p><span class="label">Table 3:</span> Configuration parameters for the different sub-generators. Time values are represented in milliseconds.
<sup>1</sup> Time distributions are based on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www2016.net/proceedings/companion/p873.pdf">public route planning logs</a> <span class="references">[<a href="#ref-65">65</a>]</span>.
<sup>2</sup> Default delays are based on the <a href="https://transportdisruption.github.io/" class="mandatory" data-link-text="https:/​/​transportdisruption.github.io/​">Transport Disruption ontology</a>.</p>
        </figcaption>
</figure>

    </section>

    <section id="generating_evaluation">
      <h3>Evaluation</h3>

      <p>In this section, we discuss our evaluation of PoDiGG.
We first evaluate the realism of the generated datasets using a constant seed by comparing its coherence to real datasets,
followed by a more detailed realism evaluation of each sub-generator using distance functions.
Finally, we provide an indicative efficiency and scalability evaluation of the generator and discuss practical dataset sizes.
All scripts that were used for the following evaluation can be found on <a href="https://github.com/PoDiGG/podigg-evaluate" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​evaluate">GitHub</a>.
Our experiments were executed on a 64-bit Ubuntu 14.04 machine with 128 GB of memory and a 24-core 2.40 GHz CPU.</p>

      <h4 id="generating_subsec:evaluation:coherence">Coherence</h4>

      <h5 id="metric">Metric</h5>
      <p>In order to determine how closely synthetic RDF datasets resemble their real-world variants in terms of <em>structuredness</em>,
the coherence metric <span class="references">[<a href="#ref-48">48</a>]</span> can be used.
In RDF dataset generation, the goal is to reach a level of structuredness similar to real datasets.
As mentioned before in <a href="#generating_related-work">Subsection 3.3</a>, many synthetic datasets have a level of structuredness that is higher than their real-world counterparts.
Therefore, our coherence evaluation should indicate that our generator is not subject to the same problem.
We have implemented a <a href="https://github.com/PoDiGG/graph-coherence" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​graph-​coherence">command-line tool</a>
to calculate the coherence value for any given input dataset.</p>

      <h5 id="results">Results</h5>
      <p>When measuring the coherence of the Belgian railway, buses and Dutch railway datasets,
we discover high values for both the real-world datasets and the synthetic datasets, as can be seen in <a href="#generating_table:eval:coherence">Table 4</a>.
These nearly maximal values indicate that there is a very high level of structuredness in these real-world datasets.
Most instances have all the possible values, unlike most typical RDF datasets,
which have values around or below 0.6 <span class="references">[<a href="#ref-48">48</a>]</span>.
That is because of the very specialized nature of this dataset, and the fact that they originate
from GTFS datasets that have the characteristics of relational databases.
Only a very limited number of classes and predicates are used,
where almost all instances have the same set of attributes.
In fact, these very high coherence values for real-world datasets simplify the process of synthetic dataset generation,
as less attention needs to be given to factors that lead to lower levels of structuredness, such as optional attributes for instances.
When generating synthetic datasets using PoDiGG with the same number of stops, routes and connections for the three gold standards,
we measure very similar coherence values, with differences ranging from 0.08% to 1.64%.
This confirms that PoDiGG is able to create datasets with the same high level of structuredness to real datasets of these types,
as it inherits the relational database characteristics from its GTFS-centric mimicking algorithm.</p>

      <figure id="generating_table:eval:coherence" class="table">

        <table>
          <thead>
            <tr>
              <th style="text-align: left"> </th>
              <th style="text-align: right">Belgian railway</th>
              <th style="text-align: right">Belgian buses</th>
              <th style="text-align: right">Dutch railway</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: left"><strong>Real</strong></td>
              <td style="text-align: right">0.9845</td>
              <td style="text-align: right">0.9969</td>
              <td style="text-align: right">0.9862</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Synthetic</strong></td>
              <td style="text-align: right">0.9879</td>
              <td style="text-align: right">0.9805</td>
              <td style="text-align: right">0.9870</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Difference</strong></td>
              <td style="text-align: right">0.0034</td>
              <td style="text-align: right">0.0164</td>
              <td style="text-align: right">0.0008</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 4:</span> Coherence values for three gold standards compared to the values for equivalent synthetic variants.</p>
        </figcaption>
      </figure>

      <h4 id="generating_subsec:evaluation:distance">Distance to Gold Standards</h4>

      <p>While the coherence metric is useful to compare the level of structuredness between datasets,
it does not give any detailed information about how <em>real</em> synthetic datasets are in terms of their <em>distance</em> to the real datasets.
In this case, we are working with public transit feeds with a known structure,
so we can look at the different datasets aspects in more detail.
More specifically, we start from real geographical areas with their population distributions,
and consider the distance functions between stops, edges, routes and trips for the synthetic and gold standard datasets.
In order to check the applicability of PoDiGG to different transport types and geographical areas,
we compare with the gold standard data of the Belgian railway, the Belgian buses and the Dutch railway.
The scripts that were used to derive these gold standards from real-world data
can be found on <a href="https://github.com/PoDiGG/population-density-generator" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​population-​density-​generator">GitHub</a>.</p>

      <p>In order to construct distance functions for the different generator elements, we consider several helper functions.
The function in <a href="#generating_math:eval:closest">Equation 4</a> is used to determine the closest element
in a set of elements <span class="kdmath">$B$</span> to a given element <span class="kdmath">$a$</span>, given a distance function <span class="kdmath">$f$</span>.
The function in <a href="#generating_math:eval:distance">Equation 5</a> calculates the distance between all elements in <span class="kdmath">$A$</span> and all elements in <span class="kdmath">$B$</span>,
given a distance function <span class="kdmath">$f$</span>.
The computational complexity of <span class="kdmath">$\chi$</span> is <span class="kdmath">$O(\|B\| \cdot \kappa(f))$</span>,
where <span class="kdmath">$\kappa(f)$</span> is the cost for one distance calculation for <span class="kdmath">$f$</span>.
The complexity of <span class="kdmath">$\Delta$</span> then becomes <span class="kdmath">$O(\|A\| \cdot \|B\| \cdot \kappa(f))$</span>.</p>

      <figure id="generating_math:eval:closest" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\chi(a, B, f) \coloneqq \text{arg min}_{b \in B} f(a, b)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 4:</span> Function to determine the closest element in a set of elements.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta(A, B, f) \coloneqq &#92;
\dfrac{
      \sum\limits_{a \in A}{f(a, \chi(a, B, f))}
    + \sum\limits_{b \in B}{f(b, \chi(b, A, f))}
    }{\|A\| + \|B\|}
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 5:</span> Function to calculate the distance between all elements in a set of elements.</p>
        </figcaption>
      </figure>

      <h5 id="stops-distance">Stops Distance</h5>
      <p>For measuring the distance between two sets of stops <span class="kdmath">$S_1$</span> and <span class="kdmath">$S_2$</span>,
we introduce the distance function from <a href="#generating_math:stops:distance">Equation 6</a>.
This measures the distance between every possible pair of stops using the Euclidian distance function <span class="kdmath">$d$</span>.
Assuming a constant execution time for <span class="kdmath">$\kappa(d)$</span>,
the computational complexity for <span class="kdmath">$\Delta_\text{s}$</span> is <span class="kdmath">$O(\|S_1\| \cdot \|S_2\|)$</span>.</p>

      <figure id="generating_math:stops:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    \Delta_\text{s}(S_1, S_2) \coloneqq \Delta(S_1, S_2, d)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 6:</span> Function to calculate the distance between two stops.</p>
        </figcaption>
      </figure>

      <h5 id="edges-distance">Edges Distance</h5>
      <p>In order to measure the distance between two sets of edges <span class="kdmath">$E_1$</span> and <span class="kdmath">$E_2$</span>,
we use the distance function from <a href="#generating_math:eval:edges:distance">Equation 7</a>,
which measures the distance between all pairs of edges using the distance function $d_\text{e}$.
This distance function <span class="kdmath">$d_\text{e}$</span>, which is introduced in <a href="#generating_math:eval:edge:distance">Equation 8</a>,
measures the Euclidian distance between the start and endpoints of each edge, and between the different edges,
weighed by the length of the edges.
The constant <span class="kdmath">$1$</span> in <a href="#generating_math:eval:edge:distance">Equation 8</a> is to ensure that the distance between two edges that have an equal length,
but exist at a different position, is not necessarily zero.
The computational cost of <span class="kdmath">$d_\text{e}$</span> can be considered as a constant,
so the complexity of <span class="kdmath">$\Delta_\text{e}$</span> becomes <span class="kdmath">$O(\|E_1\| \cdot \|E_2\|)$</span>.</p>

      <figure id="generating_math:eval:edges:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta_\text{e}(E_1, E_2) \coloneqq \Delta(E_1, E_2, d_\text{e})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 7:</span> Function to calculate the distance between two sets of edges.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:edge:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    d_\text{e}(e_1, e_2) \coloneqq & \text{min}\big(
          d(e_1^\text{from}, e_2^\text{from})
        + d(e_1^\text{to}, e_2^\text{to}), &#92;
    &\quad\quad
          d(e_1^\text{from}, e_2^\text{to})
        + d(e_1^\text{to}, e_2^\text{from})\big) &#92;
    &\cdot (d(e_1^\text{from}, e_1^\text{to}) - d(e_2^\text{from}, e_2^\text{to}) + 1)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 8:</span> Function to calculate the distance of an edge.</p>
        </figcaption>
      </figure>

      <h5 id="routes-distance">Routes Distance</h5>
      <p>Similarly, the distance between two sets of routes <span class="kdmath">$R_1$</span> and <span class="kdmath">$R_2$</span> is measured in <a href="#generating_math:eval:routes:distance">Equation 9</a>
by applying <span class="kdmath">$\Delta$</span> for the distance function <span class="kdmath">$d_\text{r}$</span>.
<a href="#generating_math:eval:route:distance">Equation 10</a> introduces this distance function <span class="kdmath">$d_\text{r}$</span> between two routes,
which is calculated by considering the edges in each route and measuring the distance
between those two sets using the distance function <span class="kdmath">$\Delta_\text{e}$</span> from <a href="#generating_math:eval:edges:distance">Equation 7</a>.
By considering the maximum amount of edges per route as <span class="kdmath">$e_\text{max}$</span>,
the complexity of <span class="kdmath">$d_\text{r}$</span> becomes <span class="kdmath">$O(e_\text{max}^2)$</span>
This leads to a complexity of <span class="kdmath">$O(\|R_1\| \cdot \|R_2\| \cdot e_\text{max}^2)$</span> for <span class="kdmath">$\Delta_\text{r}$</span>.</p>

      <figure id="generating_math:eval:routes:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta_\text{r}(R_1, R_2) \coloneqq \Delta(R_1, R_2, d_\text{r})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 9:</span> Function to calculate the distance between two sets of routes.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:route:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
d_\text{r}(r_1, r_2) \coloneqq \Delta_\text{e}(r_1^\text{edges}, r_2^\text{edges})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 10:</span> Function to calculate the distance between two routes.</p>
        </figcaption>
      </figure>

      <h5 id="connections-distance">Connections Distance</h5>
      <p>Finally, we measure the distance between two sets of connections <span class="kdmath">$C_1$</span> and <span class="kdmath">$C_2$</span>
using the function from <a href="#generating_math:eval:connections:distance">Equation 11</a>.
The distance between two connections is measured using the function from <a href="#generating_math:eval:connection:distance">Equation 12</a>,
which is done by considering their respective temporal distance weighed by a constant <span class="kdmath">$d_\epsilon$</span> –when serializing time in milliseconds, we set <span class="kdmath">$d_\epsilon$</span> to <span class="kdmath">$60000$</span>.–,
and their geospatial distance using the edge distance function <span class="kdmath">$d_\text{e}$</span>.
The complexity of time calculation in <span class="kdmath">$d_\text{c}$</span> can be considered being constant,
which makes it overall complexity <span class="kdmath">$O(e_\text{max}^2)$</span>.
For <span class="kdmath">$\Delta_\text{c}$</span>, this leads to a complexity of <span class="kdmath">$O(\|C_1\| \cdot \|C_2\| \cdot e_\text{max}^2)$</span>.</p>

      <figure id="generating_math:eval:connections:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
\Delta_\text{c}(C_1, C_2) \coloneqq \Delta(C_1, C_2, d_\text{c})
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 11:</span> Function to calculate the distance between two sets of connections.</p>
        </figcaption>
      </figure>

      <figure id="generating_math:eval:connection:distance" class="equation">
        <p><span class="kdmath">$\begin{aligned}
    d_\text{c}(c_1, c_2) &\coloneqq ((c_1^\text{departureTime} - c_2^\text{departureTime}) &#92;
    &+ (c_1^\text{arrivalTime} - c_2^\text{arrivalTime}) / d_\epsilon) &#92;
    &+ d_\text{e}(c_1, c_2)
\end{aligned}$</span></p>
        <figcaption>
          <p><span class="label">Equation 12:</span> Function to calculate the distance between two connections.</p>
        </figcaption>
      </figure>

      <h5 id="computability">Computability</h5>
      <p>When using the introduced functions for calculating the distance between stops, edges, routes or connections,
execution times can become long for a large number of elements because of their large complexity.
When applying these distance functions for realistic numbers of stops, edges, routes and connections,
several optimizations should be done in order to calculate these distances in a reasonable time.
A major contributor for these high complexities is <span class="kdmath">$\chi$</span> for finding the closest element from a set of elements to a given element,
as introduced in <a href="#generating_math:eval:closest">Equation 4</a>.
In practice, we only observed extreme execution times for the respective distance between routes and connections.
For routes, we implemented an optimization, with the same worst-case complexity,
that indexes routes based on their geospatial position, and performs radial search around each route
when the closest one from a set of other routes should be found.
For connections, we consider the linear time dimension when performing binary search for finding the closest connection from a set of elements.</p>

      <h5 id="metrics">Metrics</h5>
      <p>In order to measure the realism of each generator phase, we introduce a <em>realism</em> factor <span class="kdmath">$\rho$</span> for each phase.
These values are calculated by measuring the distance from randomly generated elements to the gold standard,
divided by the distance from the actually generated elements to the gold standard,
as shown below for respectively stops, edges, routes and connections.
We consider these randomly generated elements having the lowest possible level of realism,
so we use these as a weighting factor in our realism values.</p>

      <div class="kdmath">$$
\begin{aligned}
    \rho_\text{s}(S_\text{rand}, S_\text{gen}, S_\text{gs})
    &\coloneqq \Delta_\text{s}(S_\text{rand}, S_\text{gs}) / \Delta_\text{s}(S_\text{gen}, S_\text{gs})&#92;
    \rho_\text{e}(E_\text{rand}, E_\text{gen}, E_\text{gs})
    &\coloneqq \Delta_\text{e}(E_\text{rand}, E_\text{gs}) / \Delta_\text{e}(E_\text{gen}, E_\text{gs})&#92;
    \rho_\text{r}(R_\text{rand}, R_\text{gen}, R_\text{gs})
    &\coloneqq \Delta_\text{r}(R_\text{rand}, R_\text{gs}) / \Delta_\text{r}(R_\text{gen}, R_\text{gs})&#92;
    \rho_\text{c}(C_\text{rand}, C_\text{gen}, C_\text{gs})
    &\coloneqq \Delta_\text{c}(C_\text{rand}, C_\text{gs}) / \Delta_\text{c}(C_\text{gen}, C_\text{gs})
\end{aligned}
$$</div>

      <h5 id="results-1">Results</h5>
      <p>We measured these realism values with gold standards for the Belgian railway, the Belgian buses and the Dutch railway.
In each case, we used an optimal set of <a href="https://github.com/PoDiGG/podigg-evaluate/blob/master/bin/evaluate.js" class="mandatory" data-link-text="https:/​/​github.com/​PoDiGG/​podigg-​evaluate/​blob/​master/​bin/​evaluate.js">parameters</a>
to achieve the most realistic generated output.
<a href="#generating_table:eval:realism">Table 5</a> shows the realism values for the three cases,
which are visualized in <a href="#generating_fig:realism:stops">Fig. 21</a>, <a href="#generating_fig:realism:edges">Fig. 25</a>, <a href="#generating_fig:realism:routes">Fig. 29</a> and <a href="#generating_fig:realism:connections">Fig. 33</a>.
Each value is larger than 1, showing that the generator at least produces data that is closer to the gold standard,
and is therefore more realistic.
The realism for edges is in each case very large, showing that our algorithm produces edges that are very similar to
actual the edge placement in public transport networks according to our distance function.
Next, the realism of stops is lower, but still sufficiently high to consider it as realistic.
Finally, the values for routes and connections show that these sub-generators produce output that is closer
to the gold standard than the random function according to our distance function.
Routes achieve the best level of realism for the Belgian railway case.
For this same case, the connections are however only slightly closer to the gold standard than random placement,
while for the other cases the realism is more significant.
All of these realism values show that PoDiGG is able to produce realistic data for different regions and different transport types.</p>

      <figure id="generating_table:eval:realism" class="table">

        <table>
          <thead>
            <tr>
              <th style="text-align: left"> </th>
              <th style="text-align: right">Belgian railway</th>
              <th style="text-align: right">Belgian buses</th>
              <th style="text-align: right">Dutch railway</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: left"><strong>Stops</strong></td>
              <td style="text-align: right">5.5490</td>
              <td style="text-align: right">297.0888</td>
              <td style="text-align: right">4.0017</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Edges</strong></td>
              <td style="text-align: right">147.4209</td>
              <td style="text-align: right">1633.4693</td>
              <td style="text-align: right">318.4131</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Routes</strong></td>
              <td style="text-align: right">2.2420</td>
              <td style="text-align: right">0.0164</td>
              <td style="text-align: right">1.3095</td>
            </tr>
            <tr>
              <td style="text-align: left"><strong>Connections</strong></td>
              <td style="text-align: right">1.0451</td>
              <td style="text-align: right">1.5006</td>
              <td style="text-align: right">1.3017</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 5:</span> Realism values for the three gold standards in case of the different sub-generators,
respectively calculated for the stops <span class="kdmath">$\rho_\text{s}$</span>, edges <span class="kdmath">$\rho_\text{e}$</span>, routes <span class="kdmath">$\rho_\text{r}$</span> and connections <span class="kdmath">$\rho_\text{c}$</span>.</p>
        </figcaption>
      </figure>

      <figure id="generating_fig:realism:stops">

<figure id="generating_fig:realism:stops:rand" class="subfigure">
<img src="generating/img/realism/train_be/stops_random.png" alt="Stops Random" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 21:</span> Random</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:stops:gen" class="subfigure">
<img src="generating/img/realism/train_be/stops_parameterized.png" alt="Stops Generated" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 23:</span> Generated</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:stops:gs" class="subfigure">
<img src="generating/img/realism/train_be/stops_gs.png" alt="Stops Gold standard" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 24:</span> Gold standard</p>
          </figcaption>
</figure>

<figcaption>
          <p>Stops for the Belgian railway case.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:realism:edges">

<figure id="generating_fig:realism:edges:rand" class="subfigure">
<img src="generating/img/realism/train_be/edges_random.png" alt="Edges Random" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 25:</span> Random</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:edges:gen" class="subfigure">
<img src="generating/img/realism/train_be/edges_parameterized.png" alt="Edges Generated" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 27:</span> Generated</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:edges:gs" class="subfigure">
<img src="generating/img/realism/train_be/edges_gs.png" alt="Edges Gold standard" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 28:</span> Gold standard</p>
          </figcaption>
</figure>

<figcaption>
          <p>Edges for the Belgian railway case.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:realism:routes">

<figure id="generating_fig:realism:routes:rand" class="subfigure">
<img src="generating/img/realism/train_be/routes_random.png" alt="Routes Random" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 29:</span> Random</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:routes:gen" class="subfigure">
<img src="generating/img/realism/train_be/routes_parameterized.png" alt="Routes Generated" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 31:</span> Generated</p>
          </figcaption>
</figure>

<figure id="generating_fig:realism:routes:gs" class="subfigure">
<img src="generating/img/realism/train_be/routes_gs.png" alt="Routes Gold standard" style="width: 30%" />
<figcaption>
            <p><span class="label">Fig. 32:</span> Gold standard</p>
          </figcaption>
</figure>

<figcaption>
          <p>Routes for the Belgian railway case.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:realism:connections">
<img src="generating/img/realism/train_be/connections_distr.svg" alt="Hourly distribution" />
<figcaption>
          <p><span class="label">Fig. 33:</span> Connections per hour for the Belgian railway case.</p>
        </figcaption>
</figure>

      <h4 id="generating_subsec:evaluation:performance">Performance</h4>

      <h5 id="metrics-1">Metrics</h5>
      <p>While performance is not the main focus of this work,
we provide an indicative performance evaluation in this section in order to discover the bottlenecks and limitations
of our current implementation that could be further investigated and resolved in future work.
We measure the impact of different parameters on the execution times of the generator.
The three main parameters for increasing the output dataset size are the number of stops, routes and connections.
Because the number of edges is implicitly derived from the number of stops in order to reach a connected network,
this can not be configured directly.
In this section, we start from a set of parameters that produces realistic output data that is similar to the Belgian railway case.
We let the value for each of these parameters increase to see the evolution of the execution times and memory usage.
\end{paragraph}</p>

      <h5 id="results-2">Results</h5>
      <p><a href="#generating_fig:performance:times">Fig. 34</a> shows a linear increase in execution times when increasing the routes or connections.
The execution times for stops do however increase much faster, which is caused by the higher complexity of networks that are formed for many stops.
The used algorithms for producing this network graph proves to be the main bottleneck when generating large networks.
Networks with a limited size can however be generated quickly, for any number of routes and connections.
The memory usage results from <a href="#generating_fig:performance:mem">Fig. 35</a> also show a linear increase,
but now the increase for routes and connections is higher than for the stops parameter.
These figures show that stops generation is a more CPU intensive process than routes and connections generation.
These last two are able to make better usage of the available memory for speeding up the process.</p>

      <figure id="generating_fig:performance:times">
<img src="generating/img/performance/times.svg" alt="Execution times" />
<figcaption>
          <p><span class="label">Fig. 34:</span> Execution times when increasing the number of stops, routes or connections.</p>
        </figcaption>
</figure>

      <figure id="generating_fig:performance:mem">
<img src="generating/img/performance/mem.svg" alt="Memory usage" />
<figcaption>
          <p><span class="label">Fig. 35:</span> Memory usage when increasing the number of stops, routes or connections.</p>
        </figcaption>
</figure>

      <h4 id="dataset-size">Dataset size</h4>

      <p>An important aspect of dataset generation is its ability to output various dataset sizes.
In PoDiGG, different options are available for tweaking these sizes.
Increasing the time range parameter within the generator increases the number of connections
while the number of stops and routes will remain the same.
When enlarging the geographical area over the same period of time, the opposite is true.
As a rule of thumb, based on the number of triples per connection, stops and routes,
the total number of generated triples per dataset is approximately <span class="kdmath">$7 \cdot \textit{\#connections} + 6 \cdot \textit{\#stops} + \textit{\#routes}$</span>.
For the Belgian railway case, containing 30,011 connections over a period of 9 months,
with 583 stops and 362 routes, this would theoretically result in 213,937 triples.
In practice, we reach 235,700 triples when running with these parameters, which is slightly higher because of the other triples that are not
taken into account for this simplified formula, such as the ones for trips, stations and delays.</p>

    </section>

    <section id="generating_discussion">
      <h3>Discussion</h3>

      <p>In this section, we discuss the main characteristics, the usage within benchmarks and the limitations of this work.
Finally, we mention several PoDiGG use cases.</p>

      <h4 id="characteristics">Characteristics</h4>

      <p>Our main research question on how to generate realistic synthetic public transport networks
has been answered by the introduction of the mimicking algorithm from <a href="#generating_methodology">Subsection 3.6</a>,
based on commonly used practises in transit network design.
This is based on the accepted hypothesis that the population distribution
of an area is correlated with its transport network design and scheduling.
We measured the realism of the generated datasets using the coherence metric in <a href="#generating_subsec:evaluation:coherence">Subsubsection 3.8.1</a>
and more fine-grained realism metrics for different public transport aspects in <a href="#generating_subsec:evaluation:distance">Subsubsection 3.8.2</a>.</p>

      <p>PoDiGG, our implementation of the algorithm, accepts a wide range of parameters to configure the mimicking algorithm.
PoDiGG and PoDiGG-LC are able to output the mimicked data respectively as GTFS and RDF datasets,
together with a visualization of the generated transit network.
Our system can be used without requiring any extensive setup or advanced programming skills,
as it consists of simple command lines tools that can be invoked with a number of optional parameters to configure the generator.
Our system is proven to be generalizable to other transport types,
as we evaluated PoDiGG for the bus and train transport type,
and the Belgium and Netherlands geospatial regions in <a href="#generating_subsec:evaluation:distance">Subsubsection 3.8.2</a>.</p>

      <h4 id="usage-within-benchmarks">Usage within Benchmarks</h4>

      <p>A~synthetic dataset generator,
which is one of the main contributions of this work,
forms an essential aspect of benchmarks for (RDF) data management systems <span class="references">[<a href="#ref-61">61</a>, <a href="#ref-66">66</a>]</span>.
Prescribing a concrete benchmark that includes the evaluation of tasks is out of scope.
However,
to provide a guideline on how our dataset generator can be used as part of a benchmark,
we relate the primary elements of public transport datasets to <em>choke points</em> in data management systems,
i.e., key technical challenges in these system.
Below, we list choke points related to <em>storage</em> and <em>querying</em> within data management systems and route planning systems.
For each choke point, we introduce example tasks to evaluate them in the context of public transport datasets.
The querying choke points are inspired by the choke points identified by <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/3132218.3132242">Petzka et. al. for faceted browsing</a> <span class="references">[<a href="#ref-67">67</a>]</span>.</p>

      <ol>
        <li>Storage of entities.
          <ol>
            <li>Storage of stops, stations, connections, routes, trips and delays.</li>
          </ol>
        </li>
        <li>Storage of links between entities.
          <ol>
            <li>Storage of stops per station.</li>
            <li>Storage of connections for stops.</li>
            <li>Storage of the next connection for each connection.</li>
            <li>Storage of connections per trip.</li>
            <li>Storage of trips per route.</li>
            <li>Storage of a connection per delay.</li>
          </ol>
        </li>
        <li>Storage of literals.
          <ol>
            <li>Storage of latitude, longitude, platform code and code of stops.</li>
            <li>Storage of latitude, longitude and label of stations.</li>
            <li>Storage of delay durations.</li>
            <li>Storage of the start and end time of connections.</li>
          </ol>
        </li>
        <li>Storage of sequences.
          <ol>
            <li>Storage of sequences of connections.</li>
          </ol>
        </li>
        <li>Find instances by property value.
          <ol>
            <li>Find latitude, longitude, platform code or code by stop.</li>
            <li>Find station by stop.</li>
            <li>Find country by station.</li>
            <li>Find latitude, longitude, or label by station.</li>
            <li>Find delay by connection.</li>
            <li>Find next connection by connection.</li>
            <li>Find trip by connection.</li>
            <li>Find route by connection.</li>
            <li>Find route by trip.</li>
          </ol>
        </li>
        <li>Find instances by inverse property value.
          <ol>
            <li><em>Inverse of examples above.</em></li>
          </ol>
        </li>
        <li>Find instances by a combination of properties values.
          <ol>
            <li>Find stops by geospatial location.</li>
            <li>Find stations by geospatial location.</li>
          </ol>
        </li>
        <li>Find instances for a certain property path with a certain value.
          <ol>
            <li>Find the delay value of the connection after a given connection.</li>
            <li>Find the delay values of all connections after a given connection.</li>
          </ol>
        </li>
        <li>Find instances by inverse property path with a certain value.
          <ol>
            <li>Find stops that are part of a certain trip that passes by the stop at the given geospatial location.</li>
          </ol>
        </li>
        <li>Find instances by class, including subclasses.
          <ol>
            <li>Find delays of a certain class.</li>
          </ol>
        </li>
        <li>Find instances with a numerical value within a certain interval.
          <ol>
            <li>Find stops by latitude or longitude range.</li>
            <li>Find stations by latitude or longitude range.</li>
            <li>Find delays with durations within a certain range.</li>
          </ol>
        </li>
        <li>Find instances with a combination of numerical values within a certain interval.
          <ol>
            <li>Find stops by geospatial range.</li>
            <li>Find stations by geospatial range.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain value for a certain property path.
          <ol>
            <li>Find connections that pass by stops in a given geospatial range.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain value.
          <ol>
            <li>Find connections that occur at a certain time.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain value for a certain property path.
          <ol>
            <li>Find trips that occur at a certain time.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain interval.
          <ol>
            <li>Find connections that occur during a certain time interval.</li>
          </ol>
        </li>
        <li>Find instances with a numerical interval by a certain interval for a certain property path.
          <ol>
            <li>Find trips that occur during a certain time interval.</li>
          </ol>
        </li>
        <li>Find instances with numerical intervals by intervals with property paths.
          <ol>
            <li>Find connections that occur during a certain time interval with stations that have stops in a given geospatial range.</li>
            <li>Find trips that occur during a certain time interval with stops in a given geospatial range.</li>
            <li>Plan a route that gets me from stop A to stop B starting at a certain time.</li>
          </ol>
        </li>
      </ol>

      <p>This list of choke points and tasks can be used
as a basis for benchmarking spatiotemporal data management systems 
using public transport datasets.
For example, SPARQL queries can be developed based on these tasks
and executed by systems using a public transport dataset.
For the benchmarking with these tasks, it is essential that the used datasets are realistic,
as discussed in <a href="#generating_subsec:evaluation:distance">Subsubsection 3.8.2</a>.
Otherwise, certain choke points may not resemble the real world.
For example, if an unrealistic dataset would contain only a single trip that goes over all stops,
then finding a route between two given stops could be unrealistically simple.</p>

      <h4 id="limitations-and-future-work">Limitations and Future Work</h4>

      <p>In this section, we discuss the limitations of the current mimicking algorithm and its implementation,
together with further research opportunities.</p>

      <h5 id="memory-usage">Memory Usage</h5>
      <p>The sequential steps in the presented mimicking algorithm require persistence of the intermediary data that is generated in each step.
Currently, PoDiGG is implemented in such a way that all data is kept in-memory for the duration of the generation, until it is serialized.
When large datasets need to be generated, this requires a larger amount of memory to be allocated to the generator.
Especially for large amounts of routes or connections, where 100 million connections already require almost 10GB of memory to be allocated.
While performance was not the primary concern in this work, in future work, improvements could be made in the future.
A first possible solution would be to use a memory-mapped database for intermediary data,
so that not all data must remain in memory at all times.
An alternative solution would be to modify the mimicking process to a streaming algorithm,
so that only small parts of data need to be kept in memory for datasets of any size.
Considering the complexity of transit networks, a pure streaming algorithm might not be feasible,
because route design requires knowledge of the whole network.
The generation of connections however, could be adapted so that it works as a streaming algorithm.</p>

      <h5 id="realism">Realism</h5>
      <p>We aimed to produce realistic transit feeds by reusing the methodologies learned in public transit planning.
Our current evaluation compares generated output to real datasets, as no similar generators currently exist.
When similar generation algorithms are introduced in the future, this evaluation can be extended to compare their levels of realism.
Our results showed that all sub-generators, except for the trips generator, produced output with a high realism value.
The trips are still closer to real data than a random generator, but this can be further improved in future work.
This can be done by for instance taking into account <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">network capacities</a> <span class="references">[<a href="#ref-57">57</a>]</span>
on certain edges when instantiating routes as trips,
because we currently assume infinite edge capacities, which can result in a large amount of connections over an edge at the same time,
which may not be realistic for certain networks.
Alternatively, we could include other factors in the generation algorithm,
such as the location of certain points of interest, such as shopping areas, schools and tourist spots.
In the future, a study could be done to identify and measure the impact of certain points of interest on transit networks,
which could be used as additional input to the generation algorithm to further improve the level of realism.
Next to this, in order to improve <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">transfer coordination</a> <span class="references">[<a href="#ref-57">57</a>]</span>,
possible transfers between trips should be taken into account when generating stop times.
Limiting the network capacity will also lead to <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1023/A:1015231126594">natural congestion of networks</a> <span class="references">[<a href="#ref-59">59</a>]</span>,
which should also be taken into account for improving the realism.
Furthermore, the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1016/j.tra.2008.03.011">total vehicle fleet size</a> <span class="references">[<a href="#ref-57">57</a>]</span> should be considered,
because we currently assume an infinite number of available vehicles.
It is more realistic to have a limited availability of vehicles in a network,
with the last position of each vehicle being of importance when choosing the next trip for that vehicle.</p>

      <h5 id="alternative-implementations">Alternative Implementations</h5>
      <p>An alternative way of implementing this generator would be to define declarative dependency rules for public transport networks,
based on the work by <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/ITNG.2006.51">Pengyue et. al.</a> <span class="references">[<a href="#ref-60">60</a>]</span>. This would require a semantic extension to the engine
so that is aware of the relevant ontologies and that it can serialize to one or more RDF formats.
Alternatively, machine learning techniques could be used
to automatically learn the structure and characteristics of real datasets
and create <a property="schema:citation http://purl.org/spar/cito/cites" href="https://doi.org/10.1109/MIC.2008.55">similar realistic synthetic datasets</a> <span class="references">[<a href="#ref-68">68</a>]</span>,
or to <a property="schema:citation http://purl.org/spar/cito/cites" href="http://arxiv.org/abs/1609.08764">create variants of existing datasets</a> <span class="references">[<a href="#ref-69">69</a>]</span>.
The downside of machine learning techniques is however that it is typically more difficult to tweak parameters of automatically learned models
when specific characteristics of the output need to be changed, when compared to a manually implemented algorithm.
Sensitivity analysis could help to determine the impact of such parameters in order to understand the learned models better.</p>

      <h5 id="streaming-extension">Streaming Extension</h5>
      <p>Finally, the temporal aspect of public transport networks is useful for the domain of <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf">RDF stream processing</a> <span class="references">[<a href="#ref-70">70</a>]</span>.
Instead of producing single static datasets as output, PoDiGG could be adapted to produce RDF streams of connections and delays,
where information about stops and routes are part of the background knowledge.
Such an extension can become part of a benchmark, such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf">CityBench</a> <span class="references">[<a href="#ref-71">71</a>]</span> and LSBench <span class="references">[<a href="#ref-56">56</a>]</span>,
for assessing the performance of RDF stream processing systems with temporal and geospatial capabilities.</p>

      <h4 id="podigg-in-use">PoDiGG In Use</h4>

      <p>PoDiGG and PoDiGG-LC have been developed for usage within the \hobbit platform.
This platform is being developed within the HOBBIT project and aims to provide
an environment for benchmarking RDF systems for Big Linked Data.
The platform provides several default dataset generators, including PoDiGG,
which can be used to benchmark systems.</p>

      <p>PoDiGG, and its generated datasets are being used in the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://svn.aksw.org/papers/2017/ESWC_2017_MOCHA/public.pdf">ESWC Mighty Storage Challenge 2017 and 2018</a> <span class="references">[<a href="#ref-72">72</a>]</span>.
The first task of this challenge consists of RDF data ingestion into triple stores,
and querying over this data.
Because of the temporal aspect of public transport data in the form of connections,
PoDiGG datasets are fragmented by connection departure time, and transformed to a data stream that can be inserted.
In task 4 of this challenge, the efficiency of <a property="schema:citation http://purl.org/spar/cito/cites" href="http://doi.acm.org/10.1145/3132218.3132242">faceted browsing solutions is benchmarked</a> <span class="references">[<a href="#ref-67">67</a>]</span>.
In this work, a list of choke points are identified regarding SPARQL queries on triple stores,
which includes points such as the selection of subclasses and property-path transitions.
Because of the geographical property of public transport data, PoDiGG datasets are being used for this benchmark.</p>

      <p>Finally, PoDiGG is being used for creating virtual transit networks of variable size
for the purposes of benchmarking route planning frameworks, such as Linked Connections <span class="references">[<a href="#ref-49">49</a>]</span>.</p>

    </section>

    <section id="generating_conclusions">
      <h3>Conclusions</h3>

      <p>In this article, we introduced a mimicking algorithm for public transport data,
based on steps that are used in real-world transit planning.
Our method splits this process into several sub-generators and uses population distributions of an area as input.
As part of this article, we introduced PoDiGG, a reusable framework that accepts a wide range of parameters to configure the generation algorithm.</p>

      <p>Results show that the structuredness of generated datasets are similar to real public transport datasets.
Furthermore, we introduced several functions for measuring the realism of
synthetic public transport datasets compared to a gold standard on several levels, based on distance functions.
The realism was confirmed for different regions and transport types.
Finally, the execution times and memory usages were measured when increasing the most important parameters,
which showed a linear increase for each parameter, showing that the generator is able to scale to large dataset outputs.</p>

      <p>The public transport mimicking algorithm we introduced, with PoDiGG and PoDiGG-LC as implementations,
is essential for properly benchmarking the efficiency and performance
of public transport route planning systems under a wide range of realistic, but synthetic circumstances.
Flexible configuration allows datasets of any size to be created
and various characteristics to be tweaked to achieve highly specialized datasets for testing specific use cases.
In general, our dataset generator can be used for the benchmarking of geospatial and temporal RDF data management systems,
and therefore lowers the barrier towards more efficient and performant systems.</p>
    </section>

    <div class="subfooter">
  <section id="generating_acknowledgements">
        <h3 class="no-label-increment">Acknowledgements</h3>

        <p>We wish to thank Henning Petzka for his help with discovering issues and providing useful suggestions for the PoDiGG implementation.
The described research activities were funded by the H2020 project HOBBIT (#688227).</p>

      </section>

</div>
  </section>
  
  <section id="storing">
    <h2>Storing Evolving Data</h2>

    <p class="todo">Write me: https://www.rubensworks.net/publications/taelman_jws_ostrich_2018/</p>

  </section>


  <section id="querying-evolving">
    <h2>Querying Evolving Data on the Web</h2>

    <p class="todo">Write me: https://www.rubensworks.net/publications/taelman_mepdaw_bp_2016/</p>

  </section>


  <section id="summary">
    <h2>Conclusions</h2>

    <p class="todo">Write me</p>

  </section>

</main>

<footer><section id="references">
<h2>References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/" typeof="schema:CreativeWork">Harris, S., Seaborne, A., Prud’hommeaux, E.: SPARQL 1.1 Query Language. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-query-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-query-20130321/</a> (2013).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="https://dx.doi.org/10.1145/1804669.1804675" typeof="schema:Article">Schmidt, M., Meier, M., Lausen, G.: Foundations of SPARQL Query Optimization. In: Proceedings of the 13th International Conference on Database Theory. pp. 4–33 (2010).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="https://dx.doi.org/10.1145/1367497.1367578" typeof="schema:Article">Stocker, M., Seaborne, A., Bernstein, A., Kiefer, C., Reynolds, D.: SPARQL Basic Graph Pattern Optimization Using Selectivity Estimation. In: Proceedings of the 17th International Conference on World Wide Web. pp. 595–604 (2008).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-642-02184-8_2" typeof="schema:CreativeWork">Erling, O., Mikhailov, I.: RDF Support in the Virtuoso DBMS. In: Pellegrini, T., Auer, S., Tochtermann, K., and Schaffert, S. (eds.) Networked Knowledge - Networked Media: Integrating Knowledge Management, New Media Technologies and Semantic Systems. pp. 7–24. Springer Berlin Heidelberg, Berlin, Heidelberg (2009).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="#fSPARQL" typeof="schema:Article">Cheng, J., Ma, Z.M., Yan, L.: f-SPARQL: A Flexible Extension of SPARQL. In: Bringas, P.G., Hameurlain, A., and Quirchmayr, G. (eds.) Database and Expert Systems Applications. pp. 487–494. Springer Berlin Heidelberg, Berlin, Heidelberg (2010).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="https://www.w3.org/DesignIssues/LinkedData.html" typeof="schema:CreativeWork">Berners-Lee, T.: Linked Data. <a href="https://www.w3.org/DesignIssues/LinkedData.html">https:/​/​www.w3.org/DesignIssues/LinkedData.html</a> (2009).</dd>
  <dt id="ref-7">[7]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/" typeof="schema:CreativeWork">Feigenbaum, L., Todd Williams, G., Grant Clark, K., Torres, E.: SPARQL 1.1 Protocol. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-protocol-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-protocol-20130321/</a> (2013).</dd>
  <dt id="ref-8">[8]</dt>
  <dd resource="https://dx.doi.org/10.1016/j.websem.2016.03.003" typeof="schema:Article">Verborgh, R., Vander Sande, M., Hartig, O., Van Herwegen, J., De Vocht, L., De Meester, B., Haesendonck, G., Colpaert, P.: Triple Pattern Fragments: a Low-cost Knowledge Graph Interface for the Web. Journal of Web Semantics. 37–38, (2016).</dd>
  <dt id="ref-9">[9]</dt>
  <dd resource="http://olafhartig.de/files/Hartig_QueryingLD_DBSpektrum_Preprint.pdf" typeof="schema:Article">Hartig, O.: An overview on execution strategies for Linked Data queries. Datenbank-Spektrum. 13, 89–99 (2013).</dd>
  <dt id="ref-10">[10]</dt>
  <dd resource="http://linkeddatafragments.org/publications/eswc2015.pdf" typeof="schema:Article">Van Herwegen, J., Verborgh, R., Mannens, E., Van de Walle, R.: Query Execution Optimization for Clients of Triple Pattern Fragments. In: The Semantic Web. Latest Advances and New Domains (2015).</dd>
  <dt id="ref-11">[11]</dt>
  <dd resource="http://linkeddatafragments.org/publications/iswc2015-amf.pdf" typeof="schema:Article">Vander Sande, M., Verborgh, R., Van Herwegen, J., Mannens, E., Van de Walle, R.: Opportunistic Linked Data Querying through Approximate Membership Metadata. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) The Semantic Web – ISWC 2015. pp. 92–110. Springer (2015).</dd>
  <dt id="ref-12">[12]</dt>
  <dd resource="http://linkeddatafragments.org/publications/iswc2015-substring.pdf" typeof="schema:Article">Van Herwegen, J., De Vocht, L., Verborgh, R., Mannens, E., Van de Walle, R.: Substring Filtering for Low-Cost Linked Data Interfaces. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) The Semantic Web – ISWC 2015. pp. 128–143. Springer (2015).</dd>
  <dt id="ref-13">[13]</dt>
  <dd resource="http://iswc2015.semanticweb.org/sites/iswc2015.semanticweb.org/files/93660097.pdf" typeof="schema:Article">Acosta, M., Vidal, M.-E.: Networks of Linked Data Eddies: An Adaptive Web Query Processing Engine for RDF Data. In: The Semantic Web – ISWC 2015. pp. 111–127 (2015).</dd>
  <dt id="ref-14">[14]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-48472-3_48" typeof="schema:Article">Hartig, O., Buil-Aranda, C.: Bindings-Restricted Triple Pattern Fragments. In: Proceedings of the 15th International Conference on Ontologies, DataBases, and Applications of Semantics. pp. 762–779 (2016).</dd>
  <dt id="ref-15">[15]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2017/vtpf.pdf" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R., Mannens, E.: Versioned Triple Pattern Fragments: A Low-cost Linked Data Interface Feature for Web Archives. In: Proceedings of the 3rd Workshop on Managing the Evolution and Preservation of the Data Web (2017).</dd>
  <dt id="ref-16">[16]</dt>
  <dd resource="#cyclades" typeof="schema:Article">Folz, P., Skaf-Molli, H., Molli, P.: CyCLaDEs: a decentralized cache for triple pattern fragments. In: International Semantic Web Conference. pp. 455–469. Springer (2016).</dd>
  <dt id="ref-17">[17]</dt>
  <dd resource="#tpfqs" typeof="schema:Article">Taelman, R., Verborgh, R., Colpaert, P., Mannens, E.: Continuous client-side query evaluation over dynamic Linked Data. In: International Semantic Web Conference. pp. 273–289. Springer (2016).</dd>
  <dt id="ref-18">[18]</dt>
  <dd resource="#allegrograph" typeof="schema:Article">Aasman, J.: AllegroGraph: RDF triple database. Cidade: Oakland Franz Incorporated. 17, (2006).</dd>
  <dt id="ref-19">[19]</dt>
  <dd resource="#blazegraph" typeof="schema:Article">Thompson, B.B., Personick, M., Cutcher, M.: The Bigdata® RDF Graph Database. Linked Data Management. 193–237 (2014).</dd>
  <dt id="ref-20">[20]</dt>
  <dd resource="#virtuoso" typeof="schema:Chapter">Erling, O., Mikhailov, I.: Virtuoso: RDF support in a native RDBMS. In: Semantic Web Information Management. pp. 501–519. Springer (2010).</dd>
  <dt id="ref-21">[21]</dt>
  <dd resource="https://jena.apache.org/" typeof="schema:CreativeWork">Apache Jena. <a href="https://jena.apache.org/">https:/​/​jena.apache.org/</a></dd>
  <dt id="ref-22">[22]</dt>
  <dd resource="https://rdflib.readthedocs.io/en/stable/" typeof="schema:CreativeWork">RDFLib. <a href="https://rdflib.readthedocs.io/en/stable/">https:/​/​rdflib.readthedocs.io/en/stable/</a></dd>
  <dt id="ref-23">[23]</dt>
  <dd resource="https://github.com/linkeddata/rdflib.js" typeof="schema:CreativeWork">rdflib.js. <a href="https://github.com/linkeddata/rdflib.js">https:/​/​github.com/linkeddata/rdflib.js</a></dd>
  <dt id="ref-24">[24]</dt>
  <dd resource="https://github.com/antoniogarrote/rdfstore-js" typeof="schema:CreativeWork">rdfstore-js. <a href="https://github.com/antoniogarrote/rdfstore-js">https:/​/​github.com/antoniogarrote/rdfstore-js</a></dd>
  <dt id="ref-25">[25]</dt>
  <dd resource="http://arxiv.org/abs/1609.07108" typeof="schema:Article">Verborgh, R., Dumontier, M.: A Web API ecosystem through feature-based reuse. CoRR. abs/1609.07108, (2016).</dd>
  <dt id="ref-26">[26]</dt>
  <dd resource="#hydra" typeof="schema:Article">Lanthaler, M., Gütl, C.: Hydra: A Vocabulary for Hypermedia-Driven Web APIs. LDOW. 996, (2013).</dd>
  <dt id="ref-27">[27]</dt>
  <dd resource="https://linkeddatafragments.github.io/Article-Declarative-Hypermedia-Responses/" typeof="schema:Article">Taelman, R., Verborgh, R.: Declaratively Describing Responses of Hypermedia-Driven Web APIs. In: Proceedings of the 9th International Conference on Knowledge Capture (2017).</dd>
  <dt id="ref-28">[28]</dt>
  <dd resource="#publishsubscribepattern" typeof="schema:Book">Birman, K., Joseph, T.: Exploiting virtual synchrony in distributed systems. ACM (1987).</dd>
  <dt id="ref-29">[29]</dt>
  <dd resource="#actormodel" typeof="schema:Article">Hewitt, C., Bishop, P., Steiger, R.: Session 8 formalisms for artificial intelligence a universal modular actor formalism for artificial intelligence. In: Advance Papers of the Conference. p. 235. Stanford Research Institute (1973).</dd>
  <dt id="ref-30">[30]</dt>
  <dd resource="#mediatorpattern" typeof="schema:Book">Gamma, E.: Design patterns: elements of reusable object-oriented software. Pearson Education India (1995).</dd>
  <dt id="ref-31">[31]</dt>
  <dd resource="https://martinfowler.com/articles/injection.html" typeof="schema:CreativeWork">Fowler, M.: Inversion of Control Containers and the Dependency Injection pattern. <a href="https://martinfowler.com/articles/injection.html">https:/​/​martinfowler.com/articles/injection.html</a> (2004).</dd>
  <dt id="ref-32">[32]</dt>
  <dd resource="http://componentsjs.readthedocs.io/en/latest/" typeof="schema:CreativeWork">Taelman, R.: Components.js. <a href="http://componentsjs.readthedocs.io/en/latest/">http:/​/​componentsjs.readthedocs.io/en/latest/</a></dd>
  <dt id="ref-33">[33]</dt>
  <dd resource="https://linkedsoftwaredependencies.org/articles/describing-experiments/" typeof="schema:Article">Van Herwegen, J., Taelman, R., Capadisli, S., Verborgh, R.: Describing configurations of software experiments as Linked Data. In: Proceedings of the 1st Workshop on Enabling Open Semantic Science (2017).</dd>
  <dt id="ref-34">[34]</dt>
  <dd resource="#jsonld" typeof="schema:Article">Consortium, W.W.W., others: JSON-LD 1.0: a JSON-based serialization for linked data. (2014).</dd>
  <dt id="ref-35">[35]</dt>
  <dd resource="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/" typeof="schema:CreativeWork">Grant Clark, K., Feigenbaum, L., Torres, E.: SPARQL 1.1 Query Results JSON Format. W3C, <a href="https://www.w3.org/TR/2013/REC-sparql11-results-json-20130321/">https:/​/​www.w3.org/TR/2013/REC-sparql11-results-json-20130321/</a> (2013).</dd>
  <dt id="ref-36">[36]</dt>
  <dd resource="https://www.w3.org/TR/rdf-sparql-XMLres/" typeof="schema:CreativeWork">Hawke, S.: SPARQL Query Results XML Format (Second Edition). W3C, <a href="https://www.w3.org/TR/rdf-sparql-XMLres/">https:/​/​www.w3.org/TR/rdf-sparql-XMLres/</a> (2013).</dd>
  <dt id="ref-37">[37]</dt>
  <dd resource="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" typeof="schema:CreativeWork">Prud’hommeaux, E., Seaborne, A.: SPARQL Query Language for RDF. W3C, <a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/">https:/​/​www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/</a> (2008).</dd>
  <dt id="ref-38">[38]</dt>
  <dd resource="http://www.websemanticsjournal.org/index.php/ps/article/view/328" typeof="schema:Article">Fernández, J.D., Martínez-Prieto, M.A., Gutiérrez, C., Polleres, A., Arias, M.: Binary RDF Representation for Publication and Exchange (HDT). Web Semantics: Science, Services and Agents on the World Wide Web. 19, 22–41 (2013).</dd>
  <dt id="ref-39">[39]</dt>
  <dd resource="https://rdfostrich.github.io/article-demo/" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R.: OSTRICH: Versioned Random-Access Triple Store. In: Proceedings of the 27th International Conference Companion on World Wide Web (2018).</dd>
  <dt id="ref-40">[40]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-11964-9_13" typeof="schema:Article">Aluç, G., Hartig, O., Özsu, M.T., Daudjee, K.: Diversified Stress Testing of RDF Data Management Systems. In: Proceedings of the 13th International Semantic Web Conference - Part I. pp. 197–212. Springer-Verlag New York, Inc. (2014).</dd>
  <dt id="ref-41">[41]</dt>
  <dd resource="http://facebook.github.io/graphql/October2016/" typeof="schema:CreativeWork">Facebook, I.: GraphQL. Working Draft, Oct. 2016. <a href="http://facebook.github.io/graphql/October2016/">http:/​/​facebook.github.io/graphql/October2016/</a></dd>
  <dt id="ref-42">[42]</dt>
  <dd resource="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/" typeof="schema:CreativeWork">Cyganiak, R., Wood, D., Lanthaler, M.: \rdf 1.1: Concepts and Abstract Syntax. W3C, <a href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">http:/​/​www.w3.org/TR/2014/REC-rdf11-concepts-20140225/</a> (2014).</dd>
  <dt id="ref-43">[43]</dt>
  <dd resource="#linkeddata" typeof="schema:Article">Bizer, C., Heath, T., Berners-Lee, T.: Linked Data - the story so far. Semantic Services, Interoperability and Web Applications: Emerging Concepts. 205–227 (2009).</dd>
  <dt id="ref-44">[44]</dt>
  <dd resource="http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/Bizer-Schultz-Berlin-SPARQL-Benchmark-IJSWIS.pdf" typeof="schema:Article">Bizer, C., Schultz, A.: The Berlin \sparql benchmark. International Journal on Semantic Web and Information Systems. 5, 1–24 (2009).</dd>
  <dt id="ref-45">[45]</dt>
  <dd resource="#lubmbenchmark" typeof="schema:Article">Guo, Y., Pan, Z., Heflin, J.: \textscLUBM: A benchmark for \textscOWL knowledge base systems. Web Semantics: Science, Services and Agents on the World Wide Web. 3, 158–182 (2005).</dd>
  <dt id="ref-46">[46]</dt>
  <dd resource="#sp2benchmark" typeof="schema:Article">Schmidt, M., Hornung, T., Lausen, G., Pinkel, C.: \textscSP^2Bench: a \sparql performance benchmark. In: 2009 IEEE 25th International Conference on Data Engineering. pp. 222–233. IEEE (2009).</dd>
  <dt id="ref-47">[47]</dt>
  <dd resource="http://ceur-ws.org/Vol-1700/paper-02.pdf" typeof="schema:Article">Spasić, M., Jovanovik, M., Prat-Pérez, A.: An \rdf Dataset Generator for the Social Network Benchmark with Real-World Coherence. In: Fundulaki, I., Krithara, A., Ngonga Ngomo, A.-C., and Rentoumi, V. (eds.) Proceedings of the Workshop on Benchmarking Linked Data (2016).</dd>
  <dt id="ref-48">[48]</dt>
  <dd resource="#rdfbenchmarksdatasets" typeof="schema:Article">Duan, S., Kementsietsidis, A., Srinivas, K., Udrea, O.: Apples and oranges: a comparison of \rdf benchmarks and real \rdf datasets. In: Proceedings of the 2011 ACM SIGMOD International Conference on Management of data. pp. 145–156. ACM, New York, NY, USA (2011).</dd>
  <dt id="ref-49">[49]</dt>
  <dd resource="#linkedconnections" typeof="schema:Article">Colpaert, P., Llaves, A., Verborgh, R., Corcho, O., Mannens, E., Van de Walle, R.: Intermodal public transit routing using Linked Connections. In: Proceedings of the 14th International Semantic Web Conference: Posters and Demos. pp. 1–5 (2015).</dd>
  <dt id="ref-50">[50]</dt>
  <dd resource="#csa" typeof="schema:Chapter">Dibbelt, J., Pajor, T., Strasser, B., Wagner, D.: Intriguingly Simple and Fast Transit Routing. In: Bonifaci, V., Demetrescu, C., and Marchetti-Spaccamela, A. (eds.) Experimental Algorithms. pp. 43–54. Springer Berlin Heidelberg, Berlin, Heidelberg (2013).</dd>
  <dt id="ref-51">[51]</dt>
  <dd resource="https://link.springer.com/content/pdf/10.1007/978-3-642-35176-1_19.pdf" typeof="schema:Article">Kyzirakos, K., Karpathiotakis, M., Koubarakis, M.: Strabon: a semantic geospatial \dbms. In: Cudré-Mauroux, P., Heflin, J., Sirin, E., Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2012. pp. 295–311. Springer Berlin Heidelberg, Berlin, Heidelberg (2012).</dd>
  <dt id="ref-52">[52]</dt>
  <dd resource="http://www.semantic-web-journal.net/sites/default/files/swj176_3.pdf" typeof="schema:Article">Battle, R., Kolas, D.: Enabling the geospatial semantic web with parliament and \scshapegeosparql. Semantic Web. 3, 355–370 (2012).</dd>
  <dt id="ref-53">[53]</dt>
  <dd resource="http://doi.acm.org/10.1145/1526709.1526856" typeof="schema:Article">Barbieri, D.F., Braga, D., Ceri, S., Della Valle, E., Grossniklaus, M.: C-\sparql: \sparql for continuous querying. In: Proceedings of the 18th international conference on World wide web. pp. 1061–1062. ACM (2009).</dd>
  <dt id="ref-54">[54]</dt>
  <dd resource="https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_24.pdf" typeof="schema:Article">Le-Phuoc, D., Dao-Tran, M., Parreira, J.X., Hauswirth, M.: A native and adaptive approach for unified processing of Linked Streams and Linked Data. In: Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2011. pp. 370–388. Springer Berlin Heidelberg, Berlin, Heidelberg (2011).</dd>
  <dt id="ref-55">[55]</dt>
  <dd resource="http://dx.doi.org/10.1007/978-3-642-41338-4_22" typeof="schema:Article">Garbis, G., Kyzirakos, K., Koubarakis, M.: Geographica: A Benchmark for Geospatial \rdf Stores. In: Alani, H., Kagal, L., Fokoue, A., Groth, P., Biemann, C., Parreira, J.X., Aroyo, L., Noy, N., Welty, C., and Janowicz, K. (eds.) Proceedings of the 12th International Semantic Web Conference. pp. 343–359. Springer Berlin Heidelberg, Berlin, Heidelberg (2013).</dd>
  <dt id="ref-56">[56]</dt>
  <dd resource="#lsbench" typeof="schema:Article">Le-Phuoc, D., Dao-Tran, M., Pham, M.-D., Boncz, P., Eiter, T., Fink, M.: Linked stream data processing engines: Facts and figures. In: Cudré-Mauroux, P., Heflin, J., Sirin, E., Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., and Blomqvist, E. (eds.) The Semantic Web – ISWC 2012. pp. 300–312. Springer Berlin Heidelberg, Berlin, Heidelberg (2012).</dd>
  <dt id="ref-57">[57]</dt>
  <dd resource="https://doi.org/10.1016/j.tra.2008.03.011" typeof="schema:Article">Guihaire, V., Hao, J.-K.: Transit network design and scheduling: A global review. Transportation Research Part A: Policy and Practice. 42, 1251–1273 (2008).</dd>
  <dt id="ref-58">[58]</dt>
  <dd resource="#syntheticspatiotemporal" typeof="schema:Article">Nascimento, M.A., Pfoser, D., Theodoridis, Y.: Synthetic and real spatiotemporal datasets. IEEE Data Eng. Bull. 26, 26–32 (2003).</dd>
  <dt id="ref-59">[59]</dt>
  <dd resource="https://doi.org/10.1023/A:1015231126594" typeof="schema:Article">Brinkhoff, T.: A framework for generating network-based moving objects. GeoInformatica. 6, 153–180 (2002).</dd>
  <dt id="ref-60">[60]</dt>
  <dd resource="https://doi.org/10.1109/ITNG.2006.51" typeof="schema:Article">Lin, P.J., Samadi, B., Cipolone, A., Jeske, D.R., Cox, S., Rendon, C., Holt, D., Xiao, R.: Development of a synthetic data set generator for building and testing information discovery systems. In: Third International Conference on Information Technology: New Generations (ITNG’06). pp. 707–712. IEEE (2006).</dd>
  <dt id="ref-61">[61]</dt>
  <dd resource="#ldbc" typeof="schema:Article">Angles, R., Boncz, P., Larriba-Pey, J., Fundulaki, I., Neumann, T., Erling, O., Neubauer, P., Martinez-Bazan, N., Kotsev, V., Toma, I.: The Linked Data Benchmark Council: a graph and \rdf industry benchmarking effort. ACM SIGMOD Record. 43, 27–31 (2014).</dd>
  <dt id="ref-62">[62]</dt>
  <dd resource="https://doi.org/10.1007/978-3-642-02094-0_7" typeof="schema:Chapter">Delling, D., Sanders, P., Schultes, D., Wagner, D.: Engineering route planning algorithms. In: Lerner, J., Wagner, D., and Zweig, K.A. (eds.) Algorithmics of Large and Complex Networks: Design, Analysis, and Simulation. pp. 117–139. Springer Berlin Heidelberg, Berlin, Heidelberg (2009).</dd>
  <dt id="ref-63">[63]</dt>
  <dd resource="http://doi.acm.org/10.1145/1227161.1227166" typeof="schema:Article">Pyrga, E., Schulz, F., Wagner, D., Zaroliagis, C.: Efficient models for timetable information in public transportation systems. Journal of Experimental Algorithmics (JEA). 12, 2.4:1–2.4:39 (2008).</dd>
  <dt id="ref-64">[64]</dt>
  <dd resource="https://dx.doi.org/10.1137/1.9781611974317.2" typeof="schema:Article">Bast, H., Hertel, M., Storandt, S.: Scalable Transfer Patterns. 2016 Proceedings of the Eighteenth Workshop on Algorithm Engineering and Experiments (ALENEX). 15–29</dd>
  <dt id="ref-65">[65]</dt>
  <dd resource="http://www2016.net/proceedings/companion/p873.pdf" typeof="schema:Article">Colpaert, P., Chua, A., Verborgh, R., Mannens, E., Van de Walle, R., Vande Moere, A.: What public transit \scshapeAPI logs tell us about travel flows. In: Proceedings of the 6\textsuperscriptth \scshapeUSEWOD Workshop on Usage Analysis and the Web of Data. pp. 873–878. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland (2016).</dd>
  <dt id="ref-66">[66]</dt>
  <dd resource="#benchmarkhandbook" typeof="schema:Book">Gray, J.: Benchmark handbook: for database and transaction processing systems. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (1992).</dd>
  <dt id="ref-67">[67]</dt>
  <dd resource="http://doi.acm.org/10.1145/3132218.3132242" typeof="schema:Article">Petzka, H., Stadler, C., Katsimpras, G., Haarmann, B., Lehmann, J.: Benchmarking Faceted Browsing Capabilities of Triplestores. Proceedings of the 13th International Conference on Semantic Systems. 128–135 (2017).</dd>
  <dt id="ref-68">[68]</dt>
  <dd resource="https://doi.org/10.1109/MIC.2008.55" typeof="schema:Article">Eno, J., Thompson, C.W.: Generating synthetic data to match data mining patterns. IEEE Internet Computing. 12, (2008).</dd>
  <dt id="ref-69">[69]</dt>
  <dd resource="http://arxiv.org/abs/1609.08764" typeof="schema:Article">Wong, S.C., Gatt, A., Stamatescu, V., McDonnell, M.D.: Understanding data augmentation for classification: when to warp? In: Digital Image Computing: Techniques and Applications (DICTA), 2016 International Conference on. pp. 1–6. IEEE (2016).</dd>
  <dt id="ref-70">[70]</dt>
  <dd resource="http://www.few.vu.nl/~frankh/postscript/IEEE-IS09.pdf" typeof="schema:Article">Della Valle, E., Ceri, S., van Harmelen, F., Fensel, D.: It’s a Streaming World! Reasoning upon Rapidly Changing Information. Intelligent Systems, IEEE. 24, 83–89 (2009).</dd>
  <dt id="ref-71">[71]</dt>
  <dd resource="https://pdfs.semanticscholar.org/7537/3ee7efaca72d4894bb2c86033a2cadeef655.pdf" typeof="schema:Article">Ali, M.I., Gao, F., Mileo, A.: CityBench: a configurable benchmark to evaluate \rsp engines using smart city datasets. In: Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., d’Aquin, M., Srinivas, K., Groth, P., Dumontier, M., Heflin, J., Thirunarayan, K., and Staab, S. (eds.) International Semantic Web Conference. pp. 374–389. Springer International Publishing, Cham (2015).</dd>
  <dt id="ref-72">[72]</dt>
  <dd resource="https://svn.aksw.org/papers/2017/ESWC_2017_MOCHA/public.pdf" typeof="schema:Article">Georgala, K., Spasić, M., Jovanovik, M., Petzka, H., Röder, M., Ngomo, A.-C.N.: \scshapeMOCHA2017: The Mighty Storage Challenge at \scshapeESWC 2017. In: Dragoni, M., Solanki, M., and Blomqvist, E. (eds.) Semantic Web Challenges. pp. 3–15. Springer International Publishing, Cham (2017).</dd>
</dl>
</section>
</footer>



</body>
</html>
